A BERYSTWYTH U NIVERSITY
C OMPUTER S CIENCE AND S TATISTICS (GG34)

CS396: M INOR P ROJECT

Application of Machine Learning Techniques to
Next Generation Sequencing Quality Control

Author:
Sam Nicholls msn

Supervisor:
Dr. Amanda Clare afc

Draft

May 2, 2014

Declaration

I certify that except where indicated, all material in this thesis is the result of my own investigation and
references used in preparation of the text have been cited. The work has not previously been submitted as part
of any other assessed module, or submitted for any other degree or diploma.
Sam Nicholls
2014

Contents
Contents

ii

1

1

Introduction
1.1

1.2

Project Aims . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1

1.1.1

Analysis of Current System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1

1.1.2 Identification of Properties that affect Downstream Analysis . . . . . . . . . . . . . .
Project Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2
2

1.2.1

Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2

1.2.2

Task Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4

1.2.3

Time Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4

I

Analysis of Current System

5

2

Introduction and Background

6

2.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6

2.2

Concepts and Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6

2.2.1

DNA and Bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6

2.2.2

Next Generation Sequencing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6

2.2.3

Variants and SNPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7

2.2.4

Samples, Lanes and Lanelets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7

3

Materials and Methods

9

3.1

Input Data and Format . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9

3.1.1

"BAMcheckR’d" Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9

3.1.2

auto_qc Decision Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9

3.2

Development Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.2.1

Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

3.2.2

Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11

3.2.3

Additional External Libraries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

3.2.4

Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13

3.2.5

Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13

Contents
4

5

Pre-Implementation
4.1

Classification Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

4.2

Recovering Ratios . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

4.3

Contributions to bamcheckr . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

Frontier
5.1

5.2

5.3

6

7

19

Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
5.1.1

Purpose . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

5.1.2

Format . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

5.1.3

Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
5.2.1

Observations and Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

5.2.2

Data and Targets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

5.2.3

Class Labels and Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

5.2.4

Training and Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

5.2.5

Python Data Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
5.3.1

Class Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

5.3.2

Input Handling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

5.3.3

Storage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26

5.3.4

Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

5.3.5

Interaction with scikit-learn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28

5.3.6

Logging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29

5.4

Usage Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30

5.5

Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
5.5.1

AQCReader and BamcheckReader . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

5.5.2

Frontier Utilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

5.5.3

Frontier API . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

Results
6.1

II

15

32

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
6.1.1

Why Decision Trees? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

6.1.2

CART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

6.2

Parameter Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

6.3

Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
6.3.1

Initial Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

6.3.2

Parameter Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

6.3.3

Ignoring Warnings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

Identification of Qualitative Sample Properties
Introduction and Motivation
7.1

34
35

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
iii

Contents
7.2

7.3

8

7.2.1

GWAS and iCHIP Data Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

7.2.2

The ‘Goldilocks’ Region . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

Concepts and Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
7.3.1

Group . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

7.3.2

Length and Stride . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

7.3.3

Density . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

7.3.4

Candidate Regions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

Materials and Methods
8.1

8.2

9

Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

38

Input Data and Format . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
8.1.1

Variant Call Format . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38

8.1.2

Variant Query File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38

8.1.3

Paths File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

Development Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
8.2.1

Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

8.2.2

Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

8.2.3

Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

Goldilocks

40

9.1

9.2

Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
9.1.1

Purpose . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

9.1.2

Format . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

9.1.3

Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41

9.1.4

Pitfalls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41

Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
9.2.1

Input Sanity...? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

9.2.2

Loading Variant Query File List . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

9.2.3

Loading Variants from Input Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

9.2.4

Storage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

9.2.5

Searching for Regions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45

9.2.6

Final Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45

9.3

Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

9.4

Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

10 Results

47

11 Analysis Pipeline

49

11.1 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
11.1.1 Region Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
11.1.2 samtools index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
11.1.3 samtools mpileup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
11.1.4 samtools merge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50

iv

Contents
11.1.5 bcftools call . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53

III

Discussions and Conclusions

54

12 Current Status

55

13 Critical Evaluation

56

14 Conclusions

57

Appendix A Input Examples

60

A.1 BAMcheckR’d Example Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
A.2 auto_qc Decision Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
A.3 Goldilocks Pathsfile . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
Appendix B Output Examples

65

B.1 Frontier Log Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
Appendix C Tool Installation

68

C.1 htslib . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
C.2 bcftools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
C.3 samtools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
C.4 tabix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69

v

Chapter 1

Introduction
Over the past few years advances in genetic sequencing hardware have introduced the concept of massively
parallel DNA sequencing, allowing potentially billions of chemical reactions to occur simultaneously, reducing
both time and cost required to perform genetic analysis[1]. However, these "next-generation" processes are
complex and open to error[2], thus quality control is an essential step to assure confidence in any downstream
analyses performed.

1.1

Project Aims

The project consists of two sub-projects:
• Analysis of a current quality control system in place
• Identification of quantifiable sample properties that affect downstream analysis

1.1.1

Analysis of Current System

With the support of the Wellcome Trust Sanger Institute in Cambridge, this project works with the Human
Genetics Informatics team to investigate auto_qc[3], the institute’s current automated quality control tool.
During genetic sequencing a large number of metrics are generated to determine the quality of the data read
from the sequencing hardware itself. As part of the institute’s vertebrate sequencing pipeline[4], auto_qc is
responsible for applying quality control to samples within the pipeline by comparing a modest subset of these
metrics to simple hard-coded thresholds; determining whether a particular sample has reached a level that
requires a warning, or has exceeded the threshold and failed entirely. Whilst this catches most of the very
poor quality outputs, a large number of samples are flagged for manual inspection at the warning level; a time
consuming task which invites both inefficiency and error[5].

1

1.2 Project Method
In practice most of these manual decisions are based on inspecting a range of diagnostic plots, which suggest
that a machine learning classifier could potentially be trained on the combinations of quality control statistics
available to make these conclusions without the need for much human intervention.
The first part of the project aims to apply machine learning techniques to replicate the current auto_qc rule set
by training a decision tree classifier on a large set of these quality metrics. The idea is to investigate whether
these simple threshold based rules can be recovered from such data, or whether a new classifier would produce
different rules entirely. During this analysis it is hoped the classifier may be able to identify currently unused
quality metrics that improve labelling accuracy. An investigation on the possibility of aggregating or otherwise
reducing the dimensions of some of the more detailed quality statistics to create new parameters will also be
conducted.
The goal is to improve efficiency of quality control classification, whether by improving accuracy of pass
and fail predictions over the current system or merely being able to provide additional information to a lab
technician inspecting samples labelled with a warning to reduce arbitrary decisions.

1.1.2

Identification of Properties that affect Downstream Analysis

The other half of this project is motivated by the question "What is good and bad in terms of quality?"
To be able to classify samples as a pass or a fail with understanding, we need an idea of what actually
constitutes a good quality sample and must look at the effects quality has on analysis performed downstream
from sequencing. An example of such is variant calling – the process of identifying differences between a
DNA sample (such as your own) and a known reference sequence.
Given two high quality data sources where DNA sequences from individuals were identified in two different
ways (one of which being next-generation sequencing) it would be possible to measure the difference between
each corresponding pair. Using this, we could investigate the effect of leaving out part of the next-generation
sample during the variant calling process. If we were to leave a part of a sample out of the variant calling
pipeline would the variants found be more (or less) accurate than if it had been included? Would they agree
more (or less) with the variants called after using the non next-generation sequencing method?
Having identified such sub-samples, can quality control metrics from the previous part be found in common?
If so, such parameters would identify "good" or "bad" samples straight out of the machine. Samples that
exhibit these quality variables will go on to improve or detriment analysis.

1.2
1.2.1

Project Method
Methodology

Clearly some team-based practices invited by agile methodologies – pair programming immediately comes
to mind – are not applicable in a solo project. It is also unreasonable to expect an "on-site" customer
for this particular project. In The Case Against Extreme Programming, Matt Stephens describes a "self
2

1.2 Project Method
referential safety net" where the perceived traps in each practice are supported and "made safe" by other
extreme programming (XP) practices. This would rule out XP as a viable methodology for a solo project
as cutting out some of the processes that allow this form of evolutionary design to work (and flatten that
cost-of-change curve) can introduce serious flaws to the management of a project and potentially result
in failure. In the same breath it is important to remember that not all agile processes need be discarded
just because XP seems incompatible. Indeed, some processes are common sense, for example: frequent
refactoring, simple design, continuous integration and version control. Test driven development could also
prove a useful process to consider as part of a methodology for this project as setting up a framework that
allows for quick and frequent testing (before coding) and ensuring that any refactoring has a positive (or at
least non-negative) effect on the system could be a worthwhile contribution to efficiency.
Could a more plan driven approach or form of agile-plan hybrid be considered appropriate here? In Balancing
Agility and Discipline, Boehm and Turner introduce the idea of "homegrounds" for both agile and plan driven
approaches; noting here that for projects that require high reliability and feature a non-collocated "CRACK
customer" in fact align with some of these homegrounds for plan driven development. Combined with the
thought that the project requirements will also be relatively stable it would seem that there may be no reason
to switch to a more agile methodology as its primary feature is the welcoming of change that is not even
needed? Perhaps this is the naivety of an optimist.
Personally I think I would approach this with a form of agile-plan hybrid; I like the idea of quick iterations
and getting feedback as opposed to leaving acceptance testing until the end of the project, but I also want a
somewhat detailed feedback process. In Neil Taylor’s Agile Methodologies course it was suggested that it is
dangerous to pick and choose processes (don’t anger the Ring of Snakes) and also merely paying "lip service"
to agile must be avoided (otherwise what’s the point?), I feel that on this occasion it can be justified by the
size of the project itself.
This project will consist of many research steps, each requiring some form of computational process to prepare
the data for the next step. Whilst the implementations of the algorithms themselves pose computational
complexity, there appears to be little challenge from a planning perspective and in fact a looser overall plan
should be considered as we must account for unforeseen and unexpected outcomes from each research step.
The most important part of ensuring this project stays on track will be the development of a sensible testing
methodology to ensure we are not only moving in the right direction in terms of which algorithm and parameters
to use but also in terms of reliably measuring performance over time in a way that allows justification of such
design choices.
Despite this trail of thought, given the research grounding this project entails it might be required to look
beyond traditional and even modern software development methodologies and investigate a more scientific
approach. A simple scientific method would involve establishing a null hypothesis that can be proven false
by testing (e.g. "auto_qc classifier is more accurate than the new classifier") and executing experiments that
attempt to prove this null hypothesis false in favour of an alternative hypothesis (typically the opposite, e.g.
"The new classifier is more accurate than the old classifier"). This form of hypothesis testing could essentially
become the project’s acceptance tests (providing we have an empirical definition of what "more accurate"
means in terms of this system) and any modification can be classed as an experiment ("Do these parameters
allow us to reject the null hypothesis?"). Although care must be taken not to let this descend into unstructured
3

1.2 Project Method
cycles of mere hack-and-test, code-and-fix style programming.
Overall it is rather difficult to select a methodology for a project such as this as the research element makes it
almost impossible to draw on previous personal experience for ideas of what development processes would be
effective.

1.2.2

Task Management

It is useful to be able to keep track of current tasks preferably via a medium that would allow some method of
sorting and filtering. For various personal projects and the second year group project I have used Redmine, a
Ruby-based web application designed for bug tracking. However over time I have come to find keeping the
task information stored in Redmine a task in itself. Attempts to extend the platform to implement additional
functionality have been fruitless.
Out of frustration with thus and other alternatives – including TaskWarrior whose simple but effective
command line interface was overshadowed by its occasional storage corruptions, I wrote my own open source
web based task management application; Triage[6]. This will be useful in keeping track of current objectives
and allow prioritisation in a quick and simple manner.
The list used to organise my project is also publicly available[7] for transparent progress tracking by my
supervisor and those interested at the Sanger Institute.

1.2.3

Time Considerations

It must be remembered that this project needs to meet the requirements for a minor project and is to be
completed alongside the study of several other modules which each have their own assignments and obligations.
It would be easy to become overly ambitious and thus aims and goals will need to be revised as both obstacles
and breakthroughs are encountered over the lifetime of the project.

4

Part I

Analysis of Current System

5

Chapter 2

Introduction and Background
2.1

Introduction

This part of the project can be outlined as follows:
• Collect data sets on which a machine learning classifier is to be trained
• Construct a program capable of processing and storing such data sets such that required subsets of the
data can be quickly and easily returned for further analysis
• Select a suitable machine learning framework to handle the training and validation of a classifier
• Ensure a robust validation methodology exists for assuring quality of our own results
• Set up an environment capable of allowing results from such a classifier to be stored and compared
• Training a suitable classifier on the collected data sets
• Perform experiments by selecting subsets of the variables and observations and measure whether classification
accuracy is improved

2.2
2.2.1

Concepts and Terminology
DNA and Bases

...DNA is made up of four bases; G, C, T and A...

2.2.2

Next Generation Sequencing

...the process of recovering or extracting the sequence of nucleotide bases...
6

2.2 Concepts and Terminology

2.2.3

Variants and SNPs

...for the purpose of this study we are only interested in single nucleotide polymorphisms (SNPs) "snips" ...
...their position refers to the index of the base of the chromosome in which the polymorphism exists... ...commonly
these are 1-indexed (although this is not always the case) and thus care must be taken to ensure this is accounted
for at any step where positions are considered...
...SNPs...
variant calling - the process of identifying bases in a sample that differ from the reference genome (this is a
little simple as you also need to discern as whether the difference is actually a polymorphism or not).

2.2.4

Samples, Lanes and Lanelets

A sample is a distinct DNA specimen extracted from a particular person. For the purpose of sequencing,
samples are pipetted in to a flowcell such as the one in Figure 2.1 – a glass slide containing a series of very
thin tubules known as lanes. It is throughout these lanes that the chemical reactions involved in sequencing
take place.

Fig. 2.1 An Illumina HiSeq Flowcell[8]
Once inserted, samples are amplified in situ, in the flowcell itself. A process in which the genetic material
of each sample is caused to multiply in magnitude to form a dense cluster of the sample around the original.
Millions of clusters will be created throughout each lane of the flowcell.
Note that a lane can contain more than one sample and a sample can appear in more than one lane; this is
known as sample multiplexing and helps to ensure that the failure of a particular lane does not hinder analysis
of a sample (as it will still be sequenced as part of another lane).
The more abstract of the definitions, a lanelet is the aggregate read of all clusters of a particular sample
in a single lane. Figure 2.2 attempts to highlight examples of a this (circled in blue – not all lanelets are
highlighted). For example Lane 5 shows the four clusters (in reality there would be millions) of Sample A
combine to represent a lanelet. A lane will have as many lanelets as it does samples.

7

2.2 Concepts and Terminology

Fig. 2.2 Example of flowcell with some lanelets highlighted

8

Chapter 3

Materials and Methods
3.1
3.1.1

Input Data and Format
"BAMcheckR’d" Data

As part of the project I have been granted access to significant data sets at the Sanger Institute, unlocking
quality control data for two of the largest studies currently undergoing analysis. A wide array of quality
metrics are available for each and every lanelet that forms part of either of the two studies, totalling 13,455
files.
The files are created by samtools stats – part of a collection of widely used open-source utilities for post-processing
and manipulation of large alignments such as those produced by next-generation sequencers that are released
under the umbrella name of "SAMtools"[9] (Sequence Alignment and Map Tools). samtools stats collects
statistics from sequence data files and produces key-value summary numbers as well as more complex tab
delimited dataframes tabulating several metrics over time.
The output of samtools stats is then parsed by an in-house tool called bamcheckr1 which supplements
the summary numbers section of the samtools stats output with additional metrics that are later used by
auto_qc for classification. This process appends additional key-value pairs in the summary numbers section.
A truncated example of a "bamcheckr’d" file can be found in Appendix A.1.
It is these summary numbers that will be the main focus of our learning task.

3.1.2

auto_qc Decision Data

To use these "bamcheckr’d" files for training and testing a machine learning classifier, it is necessary to map
each file to a classification result from auto_qc. The one-to-one mapping between each input file and its label
1 Named

such as samtools stats now incorporates bamcheck and the tool is written in R

9

3.2 Development Environment
are provided by the Sanger Institute in a separate file hereafter referred to as the AQC Decision Matrix or AQC
(Decision) File.
A truncated example of such a file can be found in Appendix A.2. Only the first few columns are included
– indeed we are only interested in the lanelet and aqc fields which provide an identifier that maps the row
to a given input file and its classification by auto_qc respectively. Latter columns pertain to a breakdown of
decisions made by auto_qc which are not included in the example for confidentiality (and brevity).

3.2
3.2.1

Development Environment
Language

Python was selected for the language of the program designed to handle this vast array of input data, more
out of personal taste rather than a detailed analysis of required performance and features. From previous
experience I was happy with the performance of Python when processing large datasets in terms of both file
handling operations and storing the data in memory for later use. Python’s generous choice of both built-in
and third-party libraries have proven useful. Due to its concise and flexible nature it is possible to rapidly
develop applications and its readability eases ongoing maintenance; useful given the short time-span allocated
for this project and the possibility of others wishing to contribute to the project codebase after completion.
Whilst the choice was made primarily on preference, this is not to say other options were not considered:
a highly popular Java-based collection of data mining tools, WEKA[10] would certainly have provided a
framework for building decision tree classifiers but did not appear to offer any significant features that were
unavailable elsewhere, whilst Java itself has the added constraint of requiring a virtual machine to be installed
which could be undesirable from a performance or security standpoint when the application is deployed to
servers at the Sanger Institute.
Difficulty was also encountered finding example implementations for WEKA with most documentation and
tutorials providing information for performing analysis via the graphical "Explorer" interface instead, which
would not be appropriate for quickly setting up and repeating experiments automatically.
Given the quality data we’ll be using to train a machine learning classifier is output from the previously
mentioned R script, bamcheckr, it was worth briefly investigating the options available for R itself as the
potential of integrating the learning and predicting functions right in to the same process that outputs the data
seemed convenient.
Whilst the tree[11] and rpart[12] packages are available for constructing decision trees in R (and actually
RWeka provides an R interface to WEKA) neither appeared to be as robust as other more well-known
frameworks. Also putting it politely, the programming paradigm of R[13] is rather different to other languages
and can significantly increase development time (and frustration[14]) if one is not well versed in the patterns
and grammar of the language and it seemed best to stick to one’s comfort zone given the brief timescale for
the project.
Had performance been a critical decision factor, lower level languages such as C, C++ or even Fortran could be
10

3.2 Development Environment
used. Briefly looking at frameworks available for C++ in particular, two popular solutions include: dlib[15],
which did not support tree-based classifiers but did offer implementations of other potentially useful algorithms
such as multi-class support vector machines; and Shark[16], which supported both decision tree and random
forest classifiers. Both packages also provided a series of utility functions for performing mathematical or
statistical operations on vectors of data.

3.2.2

Framework

Having studied the Machine and Intelligent Learning module in my final year, the prospect of getting stuck
in to the deep of a machine learning algorithm was exciting. However the reality is a lot of cumulative time
and effort has gone in to the creation and optimisation of a framework, which is unlikely to be surpassed
successfully by a short-term one-person project. Thus utilisation of a third party machine learning library
seems a wise investment for the project’s codebase.
There are clearly numerous machine learning frameworks available in many languages, some of which were
touched upon in the previous section and formed part of the development environment decisions. Whilst
it is obviously unnecessary to select a framework which uses the same language as the project, it seemed
counter-intuitive to select otherwise, for the establishing of additional arbitrary output and input steps to move
data between the two environments could impede prompt experiment repeatability and introduce error.
A mixed bag of machine learning frameworks exist in Python[17], ranging from several highly active general
purpose libraries to a multitude of smaller projects that focus on one specific learning task.
I investigate two of the larger libraries; scikit-learn[18] and Orange[19], primarily as their general purpose
nature will allow exploration of various classifier solutions (given time) without the need to integrate many
packages together, but also for their built-in functionality that aids the measuring of classifier performance and
accuracy. These two packages were also selected for investigation on their recommendation from the project
supervisor.
scikit-learn is somewhat the "new kid on the block" in terms of machine learning packages for Python,
originally starting as a Google Summer of Code project in 2007 under the name of scikits.learn2 the package
was not officially published until 2010 where it quickly gained popularity and built an international team of
contributors[20].
Arguably one of the most useful features of scikit-learn is its seamless integration with the "big names" of
scientific computing in Python: NumPy[21] and SciPy[22], making heavy use of the optimised data structures
and mathematical algorithms these packages offer, not only providing improved performance against packages
that implement their own structures but also easing interaction by not requiring users to populate abstruse
structures with their data or implement such algorithms themselves.
scikit-learn offers support for a wide range of algorithms covering many different machine learning tasks,
including decision trees and random forests. Also of interest are the various utility subpackages, taking
particular note of one for measuring the performance of the various classifiers; including functions for executing
2 Named

as a SciPy Toolkit for Learning Tasks

11

3.2 Development Environment
cross-validation and generating confusion matrices; and another which assists with the selection of parameters
(features) to use in a machine learning model.
The library boasts a growing community of users and a dedicated team of developers3 who submit hundreds
of commits to the project repository each month according to Github Pulse.
Orange, like scikit-learn is designed to be a general purpose machine learning framework and provide a wide
array of tools to work with many different types of problems. Setting it apart from many of the other packages
is the inclusion of a graphical user interface (GUI) that presents drag-and-drop-eqsue access to widgets from
which users assemble a workflow to apply to their data. The GUI also allows visualisation of elements such
as decision trees or confusion matrix plots without any effort from the end user.
Despite the repository commit history stretching back to mid 90s, only recent improvements including a major
redesign of the GUI, significant changes to its object hierarchy and its 2013 academic publication have brought
more attention to the framework.
At first glance Orange appears to offer a larger collection of functions to work specifically with classification
tasks when compared to scikit-learn. Indeed it would later be discovered that it ships with features useful to
the project that have not been implemented in scikit-learn including tree pruning and printing.
However it does not feature the same integration with Python’s scientific computing packages; NumPy and
SciPy and also requires that end users input their data in a structure defined by the framework rather than a
generic data structure. Thus use of Orange would introduce more data wrangling and munging steps to ensure
that data is represented in the correct format for a classifier to perform its learning and prediction role and also
to interpret returned objects.
Although Orange affords a suitable (and in some cases better) range of functions to use for decision trees,
I was concerned at the push to use its GUI as opposed to the package API, potentially impeding experiment
repeatability by having to reset the graphical environment each time.
Despite the scikit-learn package not yet reaching a major version milestone, its feature set, interaction with
NumPy and SciPy and useful built-in subpackages led me to choose scikit-learn as the machine learning
framework for this part of the project.

3.2.3

Additional External Libraries

As discussed, the project will make use of the open source4 SciPy Stack, consisting of several core packages
including NumPy[21]: a package for efficient numerical computation which defines generic N-dimensional
array and matrix data containers; and SciPy[22], which provides a multitude of optimised numerical routines.
Whilst the stack also includes Matplotlib, a highly popular graphing package, I typically prefer to use the R
package ggplot2[25] for creating graphs. However it is useful to note that Matplotlib integrates easily with
the data structures of NumPy (and thus scikit-learn too).
3 Some

full time developer positions are funded by INRIA, the French Institute for Research in Computer Science and Automation
of the SciPy Stack are distributed under the 3-clause Modified BSD License[23][24]

4 Components

12

3.2 Development Environment
argparse is a third-party Python library5 used for specifying the arguments and options of a command line
interface via a simple API. argparse spares the developer from having to check the presence and validity of a
user’s arguments to a Python program themselves whilst also automatically generating a friendly interface to
the program for the end user based on the definitions provided by the developer.

3.2.4

Tools

git
Keeping your code base inside some form of version control is common sense,

3.2.5

Testing

As discussed in Chapter 1.2.1, testing forms a critical part of the project given the need to monitor the impact
of changes to classification accuracy as well as to ensure the program is working correctly. Ideally, execution
of a test suite should be simple and easily repeatable. Results that pertain to accuracy should also be stored
for future reference to monitor ongoing performance of the classifier.
Such requirements could be fulfilled by a continuous integration platform – a server dedicated to the building
and testing of the code contained in a centralised repository typically to which an entire team will have write
access[27]. Whilst in this scenario there will be much less "risk" from integration issues due to the single
person team size, the themes of automated building and self-testing code can still be taken on board.
Jenkins[28] is a highly popular[29] example of such a platform with which I am familiar. Although an
out-of-the-box Jenkins instance is suitable for variety of software engineering projects, it would be necessary
to invest some time to install and tweak plugins to perform actions on test results (such as failing a build that
causes accuracy to decrease). Previous experience found that more specific tasks will often require a plugin
to be authored to overcome limitations in the feature set of a more generic plugin, which given the intricacies
of the Jenkins package layout could easily turn in to a project of its own. Unfortunately, other features that
would be useful to the project including the indexing and searching of logs are somewhat lacking in Jenkins.
Online solutions such as Travis[30] and Wercker[31] are free for open source projects and could potentially
offer a quicker set up, as both services merely require a small configuration file in the root of the repository
and a hook to be registered (allowing builds to be triggered on a code push for example) before being able to
deploy a build in the cloud.
However these online services would not have been able to handle the return of build artifacts (such as logs,
graphs or dot files containing a decision tree) without some convoluted solution of uploading them to another
service or committing them to a private code repository during execution of the job – the non-persistent nature
of these nodes would cause any artifacts to be destroyed when the node is terminated at the end of the build.
5 Although

since Python 2.7, argparse has been included as part of the standard library[26]

13

3.2 Development Environment
Also, as these virtual nodes are isolated from any sort of persisting file system it would be necessary to upload
large quantities of training data any time a build and test job is to be run. This sounds rather inefficient in terms
of both bandwidth and time and would more than likely constitute unreasonable use in view of the platform’s
"fair use" terms and conditions too.
It would seem that none of the widely available solutions would provide an environment suitable for the testing
of this project. I wanted to provide my own platform for this problem but given the time constraints of the
project this was simply not practical and in the end we settled for well formatted log files which could be
searched and processed with command line tools such as awk.

14

Chapter 4

Pre-Implementation
4.1

Classification Correlation

An important consideration for statistical analysis is the relation between observations. The "bamcheckr’d"
input data described in Chapter 3.1.1 is available per lanelet, however as shown in Chapter 2.2.4 a lane may
contain more than one lanelet. Herein lies the trouble: if during a sequencing run the flowcell is somehow
subjected to abnormal conditions (e.g. a temperature increase due to an air conditioning failure) or the device
is depleted of reagents then every lane (and thus all lanelets within) will be of considerably poor quality.

Fig. 4.1 Heatmap of lanelet QC status by lane: Lanes are vertical bars with each lanelet cell coloured red to
represent a failure, yellow for a warning and grey for a pass.
In such a case there would appear to exist a relationship between the respective qualities of each lanelet in
15

4.2 Recovering Ratios
a lane as well as each lane in a sequencing run. To examine this further, an R script utilising ggplot2 was
authored to visually inspect whether correlation existed and if so to what degree that data is affected.
Figure 4.1 displays a plot of auto_qc classification for each lanelet in a lane. The plot itself is a dense heatmap
where each lane stands as a vertical bar, broken in to horizontal cells, each of which represents a lanelet that
was sequenced in that particular lane. These lanelets are colour coded using; red for failures, yellow for
warnings and grey for passes (to allow the other two classes to be more easily seen).
Therefore an unbroken vertical red line indicates that all lanelets that comprise of that line failed to pass some
aspect of the current auto_qc thresholds. In reality there are few conditions under which a lanelet would fail
irrespective of the status of the rest of the lanelets in the same lane which typically involve an error during
the preparation of the sample (an easy to spot result as it will cause poor quality across all lanelets using that
sample).
Overall there are a series of instances appearing to support correlation for whole-lane failures and warnings
but despite this there do appear to be occasions where a lanelet has failed where the remainder of the lane
has not. Having discussed this with the project supervisor and contacts at the Sanger Institute we decided to
continue to the implementation stage, agreeing that whilst some evidence of correlation between lanelets in
the same lane has emerged, we will still be able to recover parameters that will be useful to quality control and
statistical testing may be required following this analysis to describe how powerful such parameters are taking
this possible correlation in to account.
It should be noted that the proportion of failures and warnings is considerably smaller than passes and so care
will need to be taken to find a balance; for example it would not be feasible to merely discard lanelets from
lanes that have failed entirely as there’d hardly be any data on which to train a classifier. Indeed other solutions
may be possible, perhaps weighting observations which exhibit similar behaviour to other lanelets in their lane
so as to give their parameter values less priority during the construction of the classifier itself.
It is worth noting that although the plot does not make a particular distinction between lanes in the same flow
cell, lanes are sequentially identified so the red bars of thicker-width arguably display some failures across
entire flow cells.
As a final note it should be stressed that this plot should be regarded as a diagnostic rather than an experiment
with a direct conclusion. Given more time it would be useful to investigate the nature of these possible
correlations, given a lane that has failed across all lanelets: do those lanelets actually express similar quality
metrics?

4.2

Recovering Ratios

An initial scan of the summary numbers available in the "bamcheckr’d" files introduced in Chapter 3.1.1
revealed 82 different parameters. However, when comparing this parameter set to the threshold rules of
auto_qc, it appeared that some parameters were "missing".
In the pursuit of replicating the decisions of the existing system, it is ideal to have all the parameters used

16

4.2 Recovering Ratios
in the making of those decisions at hand. In particular, the missing parameters were of a normalised nature,
typically in the form of a ratio or percentage which should make them more valuable than a parameter that
just represents a raw count of some property.
It was found that these missing parameters are calculated in a step preceeding auto_qc as part of the vr-pipe
pipeline1 .
Whilst I could have attempted to set-up my own instance of vr-pipe to recover this data, speaking with
contacts at the Sanger Institute, it was decided that this would prove troublesome work; requiring an involved
deployment to a cloud based facility such as Amazon’s Elastic Compute Cloud and installation of various Perl
dependencies as well as requiring access to many controlled databases within the institute.
Most of the functions that calculated these parameters turned out to be straightforward and could easily be
ported to another application. At first it was intended to add these functions to the program authored for this
part of the project, but the Sanger Institute suggested it would be more useful to implement such functionality
in bamcheckr directly, removing some of auto_qc’s dependence on its position in vr-pipe.
# Install and include the ‘devtools’ package
install.packages("devtools")
library(devtools)
# Install package directly from Github repository
install_github("samstudio8/seq_autoqc", subdir="bamcheckr")
# Install package from local directory
install("/home/sam/Projects/seq_autoqc/bamcheckr")
Listing 1 : Installing an in-development R package with devtools

With the help of devtools[32] (see Listing 1) it was simple to develop and test contributions to bamcheckr
locally without needing to re-publish the package after changes. An additional script was added to bamcheckr’s
NAMESPACE... to recover the missing ratio and percentage based parameters...
...unfortunately, the performance of the script was poor, taking 5.5s on average and increasing to 16.1s when
enabling a complex function containing many vector operations (potentially inefficient due to my lack of R
experience)... for overlapping_base_duplicate_percentage
...utilising the program designed and implemented in the following chapter... loaded required information and
used the API to calculate the missing parameters...
...Morandat[33]
...with such an intriguing difference in performance with significantly more time it would be appealing to
explore the idiosyncrasies of each implementation ...although such an investigation would quite likely sufficiently
1 https://github.com/wtsi-hgi/vr-pipe/blob/hgi-release/modules/VRPipe/Steps/vrtrack_auto_qc_hgi_3.pm

17

4.3 Contributions to bamcheckr
generate an entire project of its own.

4.3

Contributions to bamcheckr

...R CMD BATCH issue ...Fixed a graph plotting failure.

18

Chapter 5

Frontier
This chapter introduces Frontier: the main programming effort for this part of the project. Frontier is a Python
package that serves as a data manager, providing both interfaces to read inputs into structures in memory and
to retrieve them in formats acceptable to a machine learning framework.

5.1
5.1.1

Design
Purpose

Frontier’s purpose is to supplement analysis with scikit-learn by allowing a user to read in and parameterise
data in a format that can then be used for analysis by the scikit-learn library. Frontier was designed to simplify
the process of setting up machine learning tasks and enable experiment repeatability by removing the need for
users to spend time constructing classes and functions to parse their input data and to just get on with analysis
using scikit-learn.
Initially Frontier was to act as a wrapper around scikit-learn, essentially removing the end user’s interaction
with the library and merely providing an interface for data to be passed in and some sort of classifier to be
returned. However this quite clearly limited Frontier’s audience by tying it to a particular framework and
would quickly become unmanageable in the task of providing wrappers for all aspects of an external library.
Instead it was decided that Frontier would be used alongside a user’s chosen machine learning framework,
providing a useful API to parse and extract observations and variables from input data and arrange them in
structures suitable for processing with scikit-learn.
If possible, Frontier could also handle any objects returned, displaying or logging any textual or graphical
information pertaining to a returned classifier’s accuracy to assist a user in the ongoing performance monitoring
of changes to used data subsets or parameters.

19

5.2 Concepts

5.1.2

Format

Frontier is designed as a Python package, allowing a user to import its functionality in to other programs. The
result of this project’s technical output could almost be considered as two separate entities: Frontier itself,
the package designed to ease user interaction with scikit-learn and Front, a Python script which implements
Frontier’s functionality in order to interact with scikit-learn to conduct analysis on the auto_qc data.

5.1.3

Method

.. the result of abstracting code and tools created during the use of scikit-learn for analysis of the current
auto_qc system .. initially hard-coded to suit the specific needs of the project but followed an evolutionary
design process...
designed specifically for the given learning problem, with hard coded classes and encodings...

5.2

Concepts

In this section we introduce some ideas that will assist understanding of the application’s purpose and terminology
used in the following implementation section.

5.2.1

Observations and Parameters

An observation refers to a distinct object or "thing" from which we will attempt to understand the relationship
between its known properties and classification to be able to label future observations with unknown class
based on those properties. In the context of this project, this would be a lanelet. Other documentation in the
field may use the term sample but to avoid confusion with the definition of sample introduced in Chapter 2.2.4
we will use the term observation.
The aforementioned properties or attributes of an observation will be referred to as the parameters of
that observation. Traditionally these may be described as features but it was felt that this wording may
have connotations with discrete data. Early versions of Frontier referred to parameters as regressors, using
terminology from statistical modelling. This wording was dropped to remove any confusion between regression
and classification machine learning problems.

5.2.2

Data and Targets

Data will be used somewhat generically to refer to a matrix in which observations act as rows and their
parameters act as columns. It is expected that all observations will have the same set of parameters.
For problems of a classification nature such as ours, an observation’s target is the known classification of that
particular observation. Depending on the context of the learning problem targets may be discovered manually
20

5.2 Concepts
(e.g. counting leaves from images of plants, one will need to manually count all the leaves in the image before
being able to use it for learning) or as the output from another system such as auto_qc.
In the context of this project where the objective is to begin learning the rules of auto_qc the targets refer to
whether a particular lanelet observation was labelled as a pass, fail or warn by the current system.

5.2.3

Class Labels and Codes

An observation’s target may also be known as its label. A labelled observation is said to be a member of that
label’s class. For example a lanelet that has failed auto_qc is said to be labelled as a fail and is a member of
the class of failures.
String based class labels can pose difficulties when handling data later on, not only taking more memory to
store as opposed to a type such as an integer but also introducing accidental subsetting or discarding of data
whose labels differ unexpectedly. For example are "fail", ,"fial", "FAIL" and "Failed" supposed to be members
of the same class?
Often such labels are encoded in to simpler types such as integers.

5.2.4

Training and Testing

...

5.2.5

Python Data Structures

The Python standard library offers many different data structures each with their own properties. I introduce
three such structures below to assist the reader in understanding some of the implementation decisions made
in the following section:

List
A list[34] essentially represents an ordered sequence of elements. Lists are incredibly versatile, allowing
elements of any data type, including lists and other container structures or arbitrary objects. Elements in a list
are accessed by their index (location) in the sequence. Lists permit duplicate values.
Lists are represented in memory as a contiguous block and so great expense is incurred if the list grows beyond
its bounds and must be relocated elsewhere in memory to resize to a larger contiguous block. Whilst items can
be appended to or popped from anywhere in the container, performing those actions on a location that is not at
the end of the list will require the contents of the structure to be moved in memory to maintain the contiguous
layout.
Checking whether an item is in a list requires iterating over all n elements in the container until either a match
is found or there are no elements left to check.
21

5.3 Implementation
Dictionary
A dict[35] or dictionary is a structure containing key-value pairs in which string or numeric keys1 are mapped
to an arbitrary value or object through a hashing function. Keys in a dictionary are unique; attempting to add
a key-value pair where the key already exists will overwrite the original entry.
Python dictionaries offer constant lookup access and more importantly, unlike a list, querying the structure
for whether it contains a particular key can be performed in constant time[36]. However it is important to note
that a dictionary does not maintain order, keys are not sorted alphanumerically nor are they sorted by insertion
time. Ordering is determined arbitrarily by the dict’s underlying hash table2 .

Set
Sets[37] represent a collection of elements and are similar to a dict in that they are allowed to contain any
immutable (thus hashable) elements but entries are not mapped to a value or object. Sets are forbidden from
containing duplicate objects and do not maintain order.
Like dict, checking for membership of an item in a set completes in constant time, making the structure
particularly useful for membership testing and filtering duplicates out of other structures.

5.3

Implementation

This section investigates the implementation of Frontier’s major components:
• Informing the package of the problem domain...
• Interfaces provided for reading in data
• Storage of read data in memory
• Retrieval of stored data
• Interaction with scikit-learn

5.3.1

Class Definitions

Frontier was designed to support classification machine learning problems, to adequately support this task,
the package must be aware of each of the possible classes in the problem space. Early versions of Frontier
were specifically designed for training and testing data from auto_qc and could only support encoding and
decoding of the pass, fail and warn classes.
1 Any
2 This

type can actually be used as a dictionary key providing it is immutable, including tuples[35]
is probably the question I see most often from users new to Python on StackOverflow

22

5.3 Implementation
However this implementation was clearly esoteric and held little to no further use outside the domain of the
project’s learning task. What would happen if a class label were to be added or removed in future? Most likely
many lines of code would need to be re-written to handle such cases; the package was inflexible.
To counter this, Frontier was refactored to remove hard coded label definitions enabling its use as a more
general purpose tool where users can specify the domain’s classes and their associated labels and encodings.
Listing 2 shows the definitions used by Front to work with the auto_qc data.
CLASSES = {
"pass": {
"names": ["pass", "passed"],
"code": 1,
},
"fail": {
"names": ["fail", "failed"],
"code": -1,
},
"warn": {
"names": ["warn", "warning"],
"code": 0,
},
}
Listing 2 : Class definitions for auto_qc as passed to Frontier

As per Listing 2, to define classes a user must provide a Python dict containing the following for each label:
• class String
The dictionary key is used as the canonical name of the class label
• names [String]
A list of labels that denote membership of this class
• code Integer
The encoded representation of this class (See Chapter 5.2.3)
• _recode Boolean, Private
A flag to indicate whether an API action has changed the code of this class, typically used when treating
all members of one class as a member of another – A user should never set this manually
• _count Integer, Private
The number of observations with this label, typically used when calculating weightings based on the
proportions of class sizes and outputting logging information – A user should not set this manually
This simple structure allows Frontier to be compatible with almost any classification learning task with
minimum input from the end user. Additional utilities provided by the Frontier utils subpackage use this
23

5.3 Implementation
structure to automatically classify and encode labels without user intervention with the following functions:
• classify_label
Attempt to classify a label by comparing a given string to each set of names, locating an exact match
will return the relevant canonical class label
• encode_class
Given a canonical class label, return its code
• decode_class
Given a code, return the canonical class label unless _recode is True
• count_class
Increment the _count for a particular class given its canonical label
These functions are put to use throughout Frontier and are essential for reader classes (detailed in the next
chapter) to parse, classify and then encode observation targets automatically from relevant files.

5.3.2

Input Handling

Frontier’s modular nature allows users to write their own Python classes to read data from any form of input
file or stream. Two examples of which are the classes used to read from the "bamcheckr’d files" documented
in Appendix A.1 and the auto_qc decisions matrix briefly demonstrated in Appendix A.2.
These classes are described as Readers and implement a common base class, AbstractReader which takes
care of setting up the file handler, including functions to both close and iterate over the file’s contents. It will
also automatically call its own process_file function that skips over any header lines before passing each line
in the file stripped of any newline characters to process_line.
It is expected that derived classes will at least provide their own implementations for process_line and
get_data. Failing to do so will cause Frontier to throw a NotImplementedError when attempting to use
the class to read data.
process_line defines the line handling operations that extract and store desired data found in a given line. This
responsibility includes returning None for lines that contain comments and irrelevant data and conducting any
necessary sanitisation (the BamcheckReader for example makes use of a utility function to translate spaces,
underscores and dots to hyphens).
get_data must return any read in data in a suitable structure for storage by Frontier. Typically this will be a
Python dictionary using some unique identifier for each observation as a key, mapping to an arbitrary value or
object containing that observation’s parameters. Further discussion on Frontier’s storage of data and targets is
to follow.
The AbstractReader class is designed to simplify the process of reading in observations and their targets for
end users, however it is still up to the author of the dervied class to set up any structures to store data (which
cannot be done automatically without likely enforcing potentially unhelpful constraints) before initialisation
of the inherited base class (via the call to super) as shown in Listing 3.
24

5.3 Implementation

class BamcheckReader(AbstractReader):
[...]
def __init__(self, filepath, CLASSES, auto_close=True):
self.summary = SummaryNumbers()
self.indel = IndelDistribution()
super(BamcheckReader, self).__init__(filepath, CLASSES, auto_close, 0)
[...]
Listing 3 : Extract from BamcheckReader class documenting initialisation of necessary data structures and
calling for initialisation of its inherited base class

As shown in Listing 3, the initialisation of the AbstractReader allows four arguments:
• filepath String
A relative or absolute path to a file from which to extract data or targets
• CLASSES Dictionary
A dictionary of user specified class labels defined as described in the previous chapter
• auto_close Boolean, Optional
Whether or not to close the file handle immediately after executing process_file, this is True by default
to prevent either memory leaking when users are reading in a large number of files and are perhaps
unaware that they require closing or the throwing of an IOError caused by having too many file handles
open at once
• header Integer, Optional
The number of lines to ignore before the reader should begin passing stripped lines to process_line,
defaults to 0
Currently it is also the responsibility of the author of a derived class to perform relevant sanity checking of
any extracted data. For example the BamcheckReader class checks for the presence of multiple entries of
a particular metric which will print a notice if found, unless the entries have differing values, upon which an
exception is thrown and the process is halted.
Once a reader has been defined for a particular file format, a user need only provide a directory of files to be
parsed and the name of the class designed to complete the parsing. Frontier will then take care of executing
the parsing process on all files in the directory. After a derived reader has completed file handling, Frontier
will call its get_data function to "move"3 the extracted data to its own storage.
At this point Frontier will also check the integrity of the data, primarily that all parameters have a non-zero
variance. Users will be warned when this requirement is violated; parameters with no variance cannot provide
much information for successful classification as their values are equal for all class labels!
3 Rather,

a pointer to the address of the extracted data’s dictionary hashmap will be copied to memory inside a Frontier class

25

5.3 Implementation
In future it would be useful to investigate whether it would be feasible to perform such sanity checking in a
generic manner to ensure it could be applied to a wide enough range of scenarios to make it worth including
functionality in the AbstractReader directly.
With more time, future improvements could overhaul the reader interfaces to allow users to simply specify the
format of a file in a string that can be parsed by Frontier’s IO subpackage, rather than having to write their own
derived class. Classes could also list file extensions they are capable of processing which could potentially be
used to automatically determine which readers to use without requiring the user to explicitly specify.

5.3.3

Storage

Frontier specifies a class called the Statplexer4 which provides users with a single point of access to all read in
data. The reader interfaces described in the previous chapter implement their own loading functions to populate
the _data and _targets class members of the Statplexer object, both of which are Python dictionaries.
During the parsing of observation data with the relevant reader class, the reader is expected to locate an
appropriate unique ID for each observation. In the case of processing of auto_qc data, this would be the
lanelet’s barcode which is collected from the filename of that particular lanelet’s "bamcheckr’d" file. This
identifier is then used as a key in both the _data and _targets dictionaries to map to a structure (typically
another dictionary or an arbitrary class) that stores that observation’s parameters and its known target (encoded
class label), respectively.
Although these attributes can be manipulated directly (and indeed they are for testing purposes) the leading
underscore follows a popular convention defined in Python’s style guideline, PEP8[38], where class members
with leading underscores should be treated as non-public. Python doesn’t have private variables such as those
that may be found in other languages like Java, indeed the Python style guide points out that "no attribute is
really private in Python"[38]. In an interesting StackOverflow answer on the subject, a user describes that this
is "cultural"[39] and that Python programmers are trusted not to circumvent convention and "mess around with
those private members". Users are therefore expected to use the functionality Frontier provides for controlled
getting and setting of data stored in these pseudo-private _data and _targets members.
load_data, for example, should be used exclusively when populating the two dictionaries and is automatically
called on construction of the Statplexer if the construction arguments are valid. load_data will automatically
call other necessary (pseudo-private) functions of the class including _test_variance which checks that parameter
variances are non-zero and also warns users if new observations are overwriting old ones, or if an observation
does not appear to have a corresponding target.
The following section details the retrieval of data and targets from the Statplexer which are returned to the
user in NumPy arrays. Why does Frontier not just store the read in data in such a structure to begin with?
This is primarily due to the underlying layout of a list structure as introduced in Section 5.2.5. Given the nature
of input data, the number of observations and parameters are unknown before the input data is actually read
and so memory cannot be reserved to prevent these operations. The potential number of input files renders
4A

somewhat contrived contraction of ’Statistics Multiplexer’

26

5.3 Implementation
reading through the data once to reserve the right amount of memory for a second read-through impractical in
terms of time.
It is arguable that despite this, the data could be unloaded from the _data and _targets dictionaries in to some
form of array at the end of the call to load_data. However as the Statplexer allows loading of additional data at
any time, which would not only risk increasingly expensive resizing operations as the list continues to expand
but the sanity checking that takes place in load_data involves checking for membership of a given ID in _data
and _targets which is constant for dictionaries and significantly slower for large lists (see Section 5.2.5).
It is possible that an implementation could use both a dictionary and an array; appending the observations to
the array and entering a mapping between the observation ID and its index in the array. Though this would
still not avoid the issue of relocating the array once it grows beyond its bound in memory.
Worse still, as results requested from the Statplexer API are returned to the user in a sorted matrix5 (i.e.
rows and columns are ordered alphanumerically by the observation ID and parameter name, respectively), if
observations are not processed in a sorted order (which is not a requirement) then returning results will involve
accessing the proposed _data and _targets arrays in a non-linear fashion, losing any efficiency that could be
gained from using an array in the first place.
Whilst not entirely optimal6 , in terms of a compromise between memory complexity and avoiding expensive
operations on data input and retrieval, dictionaries offer the best balance.
It should be noted that the Statplexer stores a copy of the user defined CLASSES dictionary (as the class
member _classes), which is an argument to its construction, allowing the Statplexer to share this information
with any readers or utility functions who require it.

5.3.4

Retrieval

Frontier’s Statplexer is designed to provide functions to an end user for extracting desired data in a format
suitable for passing as an argument to functions of an external framework or library, such as scikit-learn. As
specified in Frontier’s purpose, the package must provide user-friendly functions to extract observations and
parameters of interest from the internal Statplexer representation.
The previous section suggests that directly accessing elements of the Statplexer’s _data and _targets member
dictionaries, whilst possible, would violate the psuedo-private nature of the variables and even so, this would
hardly be a user friendly way in which to obtain data from the Statplexer.
As described in Section 5.1.3, development of Frontier was evolutionary, growing to meet the needs and
requirements of the underlying machine learning problem that this project strives to solve. It became clear that
in trying many combinations of observation and parameter subsets that Frontier would need to provide not just
a function to return the contents of _data and _targets but to assist in retreiving similar subsets automatically
with as little effort from the end user as possible.
5A

two-dimensional NumPy array
still require use of sorted when creating a result matrix via the API

6 Dictionaries

27

5.3 Implementation
But how can a user make an informed decision on parameters to subset? Though this is primarily the role of
feature selection, which will be discussed later in this chapter, firstly a user will need to have a feeling for what
parameters are actually present and ideally be offered functionality to allow for quick elementary exploration
of that set. For this, Frontier can be used to inspect the parameters extracted from the observations via the
functions:
• list_parameters
Return a sorted list of all parameters
• find_parameters
Given a list of input strings, return a list of parameters which contain any of those strings as a substring
• exclude_parameters
Given a list of input strings, return a list of parameters which do not contain any of the input strings as
a substring, or if needed an exact match
Once users have an idea of the parameters they wish to extract from each observation, Frontier maintains an
additional set of functions that act as a form of API, allowing users to retrieve subsets of data based on both
parameters and targets:
• get_data_by_parameters
Return data for each observation, but only include columns for each parameter in the given list
• get_data_by_target
Return data for each observation that have been classified in one of the targets specified and additionally
only return columns for the parameters in the given list
The prior section first introduces the format in which data is returned, the two-dimensional NumPy array –
referred to as the Frontier Results Matrix – in which a row represents a particular observation and each column
represents a parameter (or feature). By default both dimensions are ordered alphanumerically; rows are sorted
by their observation ID, columns by the parameter name7 . Targets are also returned in a NumPy array, with
each index of the target array mapping 1:1 with the observation row of the same index in the Results Matrix.

5.3.5

Interaction with scikit-learn

Submitting Data
...

Cross Validation
...method in which to measure classification accuracy... ...potentially use a weighting to penalise mistakes in
smaller classes...
7 Typically the ’cleaned’ or otherwise sanitised parameter name, as noted in Section 5.3.2 for example where the BamcheckReader
class will automatically convert spaces and other predefined characters to hyphens.

28

5.3 Implementation
...K fold cross validation ...using stratified K fold cross validation...

Confusion Matrices
"Normal" confusion matrix and "Warnings" confusion matrix...

5.3.6

Logging

Section 3.2.5 concluded that time constraints would not allow significant effort to be invested in building a
platform for cataloguing results from test runs of the classifier. I decided to settle for populating fields in a
structured text file, an example of which can be found in Appendix B.1.
Frontier provides functions such as write_log to dump Statplexer parameters to a given file path along
with cross-validation performance associated with a particular training and prediction run of a decision tree
classifier. Such files can easily be manipulated with common command line tools such as grep and the text
processing language, awk.
Given the time to create specialised software for storing and comparing different runs of the classifier in future,
it would be trivial to import old Frontier log files.

29

5.4 Usage Example

5.4

Usage Example

from Frontier import frontier
from Frontier.IO import DataReader, TargetReader
data_dir = "/home/sam/Projects/owl_classifier/data/"
target_path = "/home/sam/Projects/owl_classifier/targets.txt"
CLASSES = {
"hoot": {
"names": ["owl", "owls"],
"code": 1,
},
"unhoot": {
"names": ["cat", "dog", "pancake"],
"code": 0,
},
}
statplexer = frontier.Statplexer(data_dir,
target_path,
CLASSES,
DataReader,
TargetReader)
Listing 4 : Example usage of Frontier

Constructing the Statplexer requires the following arguments:
• data_dir String
Root data directory under which all files will be parsed for observation data
• target_path String
Path to file to be parsed for observation targets
• CLASSES Dictionary
A dictionary of user specified class labels defined as described in Chapter 5.3.1
• DataReader Module
Module containing the class (of the same name) to be used to parse each file in the data_dir tree for
observation data
• TargetReader Module
Module containing the class (of the same name) to be used to parse the target_path file for target data
30

5.5 Testing

5.5

Testing

Frontier is bundled with a small testing suite designed to flex the functionality of the two included readers
(AQCReader and BamcheckReader) as well as both the Frontier utilities and the Frontier API itself. The
suite consists of three Python scripts which utilise the unittest[40] package in the Python standard library.
The scripts are located in the tests directory of the Frontier package.
Listing 5 displays the commands necessary to execute each test script. Note the -m option to the Python
command instructs the interpreter to load the module named as a script.
python -m tests/frontier
python -m tests/aqcreader
python -m tests/bamcheckreader
Listing 5 Execution of the Frontier Testing Suite

5.5.1

AQCReader and BamcheckReader

Both of Frontier’s included readers are distributed with their own modest test suite, designed to test the
particular functionality of that reader.
* Test cleaning of data * Test duplicates warning * Test duplicates exception * BC Tests that some hardcoded
expected values are returned * AQC Tests that number of labels were as expected...

5.5.2

Frontier Utilities

* Test both encoding and decoding of class labels and class codes * Ensures that exceptions are thrown when
unknown class labels or codes are encountered (as this should not happen!)

5.5.3

Frontier API

* More complex * Sets up a series of test observations and parameters * Ensures each API function returns
the data expected * ...including when parameters are incorrect or unknown * ...ensures parameters should be
excluded or included... * actually found a problem with excluding! * Ensures targets map correctly to data

31

Chapter 6

Results
6.1

Introduction

6.1.1

Why Decision Trees?

6.1.2

CART

6.2

Parameter Selection

...important to find the "best" parameters ...what is best? scikit-learn uses total gini information
...frontier uses two methods:
• Backward elimination; pruning parameters with the lowest total gini
• Call scikit-learn’s SelectKBest

* incorrect degrees of freedom * warnings: /usr/lib64/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:2
RuntimeWarning: invalid value encountered in divide, causing NaN * Replaced univariate_selection with
version from master * needed use force np.float64 * ...actually data was 0... gg

32

6.3 Trees

6.3

Trees

6.3.1

Initial Trees

6.3.2

Parameter Sets

6.3.3

Ignoring Warnings

33

Part II

Identification of Qualitative Sample
Properties

34

Chapter 7

Introduction and Motivation
7.1

Introduction

The second part of the project can be outlined as follows:
• Collation of variant locations from Sanger data sets
• Construction of script to select a ‘representative’ genomic region
• Extraction of regions from whole-genome sequence data
• Establishment of leave-one-out analysis pipeline
• Comparison of concordance between pipeline results and SNP chip

7.2
7.2.1

Background
GWAS and iCHIP Data Sets

As briefly explained in Section 1.1.2 this part of the project focuses on measuring the similarity between called
variants in pairs of samples which have been sequenced in different ways. The Sanger Institute has granted
access to a set of human samples which have been sequenced twice, once with next-generation sequencing
(NGS) and the other with SNP genotyping. The resultant data sets will hereafter be referred to as:
• GWAS1 NGS
Every base in the genome of the sample has been sequenced with some level of accuracy.
• iCHIP SNP Genotyping
Only particular bases of interest in the genome of the sample has been sequenced with high accuracy.
1 Genome-Wide

Association Study

35

7.3 Concepts and Terminology
Each sample will thus have a corresponding result from each of the two studies. A result in this context refers
to the output from a variant calling pipeline at the Sanger Institute – a VCF file (introduced in Chapter 8.1)
detailing variant sites that were located during analysis of the sequenced sample.

7.2.2

The ‘Goldilocks’ Region

For the next step of the project we are looking to document what effects lanelet quality has on analysis that
occurs downstream from whole-genome sequencing. To investigate this I’ll be performing a leave-one-out
analysis on the GWAS data set: consisting of leaving a particular lanelet out, re-performing variant calling
and comparing the resulting variants called for each sample to their corresponding iCHIP variants.
The basic idea is to answer the question of what constitutes "good" or "bad" lanelet quality. Does leaving out
a lanelet from the full sequence data lead to variant calls that better match the SNP chip data, or cause the
correspondence between the two sets to decrease? In which case, having identified such lanelets, can we look
back to the quality parameters we’ve been analysing so far and find whether they have something in common
in those cases?
If we can, these parameters can be used to identify "good" or "bad" lanelets straight out of the machine. We
know that lanelets that exhibit these quality variables will go on to improve or detriment analysis.
However, variant calling is both computationally and time intensive. Whilst the Sanger Institute has significant
computing resources available, the project time scale is too small to conduct the analysis on thousands of full
sequence samples and thus we must focus on a smaller region of the human genome.
It is for this reason we are looking for what Sanger described as a "representative autosomal region". The
region must not have too many variants, or too few: a "Goldilocks genome".

7.3

Concepts and Terminology

In the search for the aforementioned "Goldilocks genome", we define the following terminology:

7.3.1

Group

Rather than considering the location of all variants in our samples as a whole, we may wish to consider how
variants are distributed across the two different data sets we have, namely the GWAS and iCHIP sets. Thus
variants will be considered as members of a group. The group may be arbitrarily defined but for the purpose
of our own analysis the set in which a variant appears will be considered as its group.
A particular variant location may appear in more than one group.

36

7.3 Concepts and Terminology

7.3.2

Length and Stride

The length of a region represents the number of bases included in the region and the stride refers to the
number of bases to be used as an offset between the first base of one region and the beginning of the next.
Sensible choices for these parameters will need to be selected after considering both time and memory.

7.3.3

Density

The density of a region represents the number of variants located within that region. Density will be recorded
for each group of variants.

7.3.4

Candidate Regions

A candidate is a region of length bases whose first base falls on a stride. A candidate region will contain
some count (described as the density) of variants for each group. In the context of this project we consider the
number of variant locations that fall within this region for both the GWAS and iCHIP data sets.
Candidates must meet all (any any additional) criteria to be considered. For example, regions whose first
position falls on a base in such a way that the chromosome ends before a region of length can be constructed
will violate the size criteria.

37

Chapter 8

Materials and Methods
8.1
8.1.1

Input Data and Format
Variant Call Format

Variant Call Format (VCF) is a (typically compressed) text file... ...containing records ...developed for the
1000 Genomes Project

8.1.2

Variant Query File

Named such for the command that creates it, the Variant Query File (or Query File) is an input format
for populating the candidate searching script with variants. The file is formed of line-delimited, tab-seperated
entries where the first field consists of a colon-delimited pair representing a chromosome and position respectively
(e.g. 1:5000 is the base at position 5000 on the first chromosome).
The Query File is generated by the vcf-query command (demonstrated in Listing 6) which is a member of
the vcftools collection – a tool set designed... specifically... for handling VCF files.
...although having found this tool, it turns out that bcftools implements a more performance efficient query
function...with exactly the same as shown in Listing 7.
Note that information on downloading and installing both tool sets can be found in Appendix C.
vcf-query cd.ichip.vcf.gz -f ’%CHROM:%POS\t%REF\t%ALT\n’
Listing 6 : Extracting variant positions from a VCF file with vcftools

http://vcftools.sourceforge.net/perl_module.html http://www.biostars.org/p/51076/ http://vcftools.sourceforge.net/htslib.html#qu

38

8.2 Development Environment

bcftools query -f ’%CHROM:%POS\t%REF\t%ALT\n’ cd-seq.vcf.gz > cd-seq.vcf.gz.q
# ! A faster alternative to using vcftools exists in bcftools ! #
# Complains about lack of tabix index but still makes the file...
Listing 7 : Extracting variant positions from a VCF file with bcftools

8.1.3

Paths File

...esoteric file format to handle grouping of Variant Query files... ...very simple, lines beginning with a ’#’
delimit the desired groups, any line following a group line is considered to be a member of that group...
...entries are tab-delimited, with two fields; a unique slug for simple identification of the file and its actual path
(relative or absolute) respectively...

8.2
8.2.1

Development Environment
Language

Once again, Python was the language of choice for this script... ...for many of the reasons previously discussed
in Chapter 3.2.1 ...we needed this quickly, Python is useful for fast prototyping... ...expected reasonable
performance without the need for a lower level language like C...

8.2.2

Testing

...a little more difficult ...discussed more at length in the following chapter...

8.2.3

Tools

As before... (Chapter 3.2.4)

39

Chapter 9

Goldilocks
This chapter introduces Goldilocks; the first programming hurdle for this part of the project. Goldilocks is
a Python script designed specifically to read in and store locations of user-selected variants on the human
genome and to then find regions of a given size on the genome where variants are of a desired density.

9.1
9.1.1

Design
Purpose

Expanding on the introduction, Goldilocks is responsible for parsing input files that contain chromosome and
base position entries... ... ... Input files can be grouped, pooling the set of unique listed variants between the
files and treating them as one search space.
For each group, a list of positions for each chromosome is stored in memory. Once all variants have been
loaded Goldilocks is designed to iterate over each chromosome1 , load all of the variants from the lists in to
an array and essentially count the number of variants present (in each group) between a given start and end
position, determined by the arguments specified by the user as described in Section 7.3.2.
This combination of a start and end position and the counts of variants present inbetween forms the structure
of a candidate. Having performed this search over all the autosomes, candidates can then be ranked by
user-specified criteria regarding the density of the counted variants.

9.1.2

Format

Goldilocks is a modestly sized but elegantly complex Python script which can also serve as a stand-alone
module to be imported in to other programs. Whilst the use case is certainly more esoteric than Frontier, I
1 Strictly

speaking we are only handling the 22 autosomes and ignoring the remaining 2 sex chromsomes (allosomes)

40

9.2 Implementation
made the choice to modularise the functionality, in the hope that if an end user were to find some function of
the script particularly useful they may call it as part of their own program.
With more time it would be interesting to research other potential uses for such a script...

9.1.3

Method

Development was akin to a more traditional methodology; requirements were simple and available up front,
testing could wait until all the components had been completed to properly measure whether outcomes were
as expected.
Goldilocks had to be constructed quickly but carefully. Although avoiding a quick and dirty solution, the
initial codebase was arguably cluttered with blocks of repeated code to perform actions for different hard
coded groups. Testing was not automated and utilised eyeballing of resulting data and occasional checking of
input files to ensure results were as expected.
The first completed version consisted of a somewhat monolithic script, prime for breaking up in to a more
organised set of functions...

9.1.4

Pitfalls

...chromosome six must be ignored... ...chromosomes are 1-indexed!
The task poses an interesting problem in terms of complexity and memory, as the human genome is over three
billion bases long in total which can easily lead to data handling impracticalities.

9.2

Implementation

...we investigate the following implementation stories:
• Handling user input of grouped Variant Query files
• Loading variants from identified Variant Query files
• Storage of read data in memory (and troubles thereof)...
• Striding and window size and data structures thereof
• Final output and selection, counters and buckets (avoid chr6)
• Testing, generate output files, call functions akin to API

41

9.2 Implementation

9.2.1

Input Sanity...?

Having recovered from the miscommunication that led me to attempt to find reverse strands in VCF files with
already known errors, I’ve been making performance improvements to the script that finds candidate genomic
regions.

9.2.2

Loading Variant Query File List

Goldilocks requires the user provide their own valid "pathsfile", meeting the specification demonstrated in
Section 8.1.3 (an actual example of such a file can be found in Appendix A.3).
merely lists locations of input files and groups them together as desired ...all files in a particular group will be
considered as a whole (all variants from all files in the group will be added to the set of chromosomes for that
group)
load_variant_files is a simple function to read the pathsfile and initialise the relevant data structures for each
group... ...each group will have a three data structures created...

9.2.3

Loading Variants from Input Files

load_variants_from_file is responsible for loading the file referenced by a particular path and adding each
valid variant record to the correct file group...
...this function also keeps track of the "furthest along" variant for each chromosome over all groups, this is
used in the search step later to ensure each group has the same number of regions...
...no duplicate checking to avoid needing to check for list membership ...this is not an issue later (setting a flag
to 1 multiple times will cause no effect)... seemed faster to repeat those actions than to check for membership...
...however this process of appending variants to a list would suffer from expensive memory moving operations
and is ripe for alteration... ...could use a dictionary... ...but perhaps a set would be more appropriate...
...investigating the difference in performance would be of interest...
...although the input for this part of the study handles <x> variants in <y> seconds and thus time was better
spent elsewhere...

9.2.4

Storage

As I discussed previously, on the hunt for my Goldilocks genomic region, it is important to consider both time
and memory as it is simple to deliver a poor performing solution.
Searching for candidates regions over the entire genome at once is probably unwise. Luckily, since our
candidate must not span chromosomes (telomeres – the ends of chromosomes are not very good for reads)
then we can easily yield a great improvement from processing chromosomes individually.
42

9.2 Implementation
The process is to extract the locations of all the variants across the genome from the SNP chip VCF files (these
files list the alleles for each detected variant for each sample), load them in to some sort of structure, “stride”
over each chromosome (with some stride offset) and finally list the variants present between the stride start
and stride start plus the desired length of the candidate. These are our regions.
Due to the use of striding, one does not simply walk the chromosome. A quick and dirty solution would be to
just look up variants in a list:

for chromosome in autosomes:
for start in range(1, len(chromosome), STRIDE):
for position in range(start, start+LENGTH):
if position in variant_position_list:
# Do something...
This is of course rather dumb. Looking up a list has O(n) performance where n is the number of variants
(and there are a lot of variants). Combining this with the sheer number of lookups performed, with the default
parameters the list would be queried half a billion times for the first chromosome alone.
You could improve this somewhat by dividing the variant_position_list in to a list of variants for each chromosome,
so at least n is smaller. It is pretty doubtful that this would make any useful impact given the number of lookups.
I’m happy to say I bypassed this option but thought it would be fun to consider its performance.
A far more sensible solution is to replace the list with a dictionary whose lookup is amortized to a constant
time. The number of lookups is still incredibly substantial but a constant lookup is starting to make this sound
viable. Using the variant positions as keys of a dictionary (in fact let us use the previous minor suggestion and
have a dictionary for each chromosome) we have:

for chromosome in autosomes:
for start in range(1, len(chromosome), STRIDE):
for position in range(start, start+LENGTH):
if position in variant_position_dict[chromosome]:
# Do something...
This still takes half an hour to run on my laptop though, surely there is a better way. I wondered if whether
dropping the lookup would improve things. Instead, how about an area of memory is allocated to house the
current chromosome and act as a variant “mask” – essentially an array where each element is a base of the
current chromosome and the value of 1 represents a variant at that position and a 0 represents no variant.

for chromosome in autosomes:
chro = np.zeros(len(chromosome), np.int8)
for variant_loc in snps_by_chromosome[chromosome]:
chro[variant_loc] = 1

43

9.2 Implementation

for start in range(1, len(chromosome), STRIDE):
for position in range(start, start+LENGTH):
if chro[position] == 1:
# Do something...
We of course have to initially load the variants in to the chromosome array, but this need only be done once
(per chromosome) and is nothing compared to the billions of lookups of the previous implementation.
Rather to my surprise this was slow (maybe if I have time I should check how it compared to looking up with
a list). Was it the allocation of memory? Perhaps I had run out of RAM and most of the work was going in to
fetching and storing data in swap?
I switched out the comparison (== 1) step for the previous dictionary based lookup and the performance
improved considerably. What was going on? There must be more to looking at a given element in the numpy
chromosome array, but what?
After a brief spell of playing with Python profilers and crashing my laptop by allocating considerably more
memory than I had with numpy.zeroes, I read the manual (!) and discovered that numpy.zeroes returns an
ndarray which uses “advanced indexing” even for single element access, effectively copying the element to a
Python scalar for comparison.
That’s a lot of memory action.
It then occurred to me that we’re interested in just how many variants are inside each region and our chromosome
is handily encapsulated in a numpy array. Why don’t we just sum together the elements in each region?
Remember variant positive base positions are 1 and 0 otherwise, useful. So the work boils down to some
clever vector mathematics calculated by numpy.
We don’t even lose any detail because the actual list of variants inside a region can be recovered in a small
amount of time just given the start and end position of the region.

for chromosome in autosomes:
chro = np.zeros(len(chromosome), np.int8)
for variant_loc in snps_by_chromosome[chromosome]:
chro[variant_loc] = 1
for start in range(1, len(chromosome), STRIDE):
num_variants = np.sum(chro[start:start+LENGTH])
# Do something...
With conservative testing this runs at least 60 times faster than before, the entirety of the human genome can
be analysed for candidate regions in less than twenty seconds (with the first few seconds taken reading in all
the variant locations in the first place).

44

9.2 Implementation
...Ignoring empty regions etc.

def load_chromosome(self, size, locations):
chro = np.zeros(size+1, np.int8)
# Populate the chromosome array with 1 for each position a variant exists
for variant_loc in locations:
chro[variant_loc] = 1
return chro
load_chromosome returns a numpy array representing the chromosome, given its desired length and a list of
variants...
...supervisor suggested bloom filters... ...would be a useful experiment to measure what further performance
gains could be acheived with such a method...

9.2.5

Searching for Regions

..
search_regions ...possibly a misnomer as this is more of a census than a search... ...responsible for the core
objective function as outlined in Section 9.1.1... ..each region of length, starting at stride is checked for the
number of variants contained therein...
...it is here that chromosome six is discarded...

9.2.6

Final Selection

initial_filter enrich
...provides functions to rank all candidate regions by density of variants ...currently criteria are still hard coded
to meet the needs of the project and propose little re-use value unless altered... ...for the purpose of this project
we’re looking at: ...the median of the GWAS (representative) ...the maximum of the iCHIP (enriching for
maximum comparisons between data sets)
...with more time it would be useful to open these functions to input from an end user to provide their own
criteria...
...in this state does however still allow for repeatable results in future with different data sets or additional
groups...

45

9.3 Usage

9.3

Usage

...

9.4

Testing

...particularly difficult. ...required generation of input files that would contain known regions to test were
found by the program... ...needed to re-write large aspects of the scripts to allow unit-like testing to take
place... ...wanted to avoid a monolithic test class...
...tests are still rather modest and with more time it would be appropriate to improve their function but I have
faith in their results...

46

Chapter 10

Results
WND
234
1074
3125
5222
880
3560
4407
1036
2734
3426
15
365
3415
1581
3184
3554
1580
414
1948
2055
2215
384
959
4214
320

GWAS
297
294
298
294
293
299
299
292
300
300
291
301
301
290
302
290
289
288
288
304
288
287
306
286
307

iCHIP
470
1540
310
336
344
772
512
300
515
486
1029
487
419
802
449
403
603
346
1297
1377
622
827
406
393
620

CHR
1
3
10
21
2
12
15
3
9
11
1
1
11
4
10
12
4
1
5
5
7
1
2
14
1

POSITIONS
117000001 - 118000000
46000001 47000000
60000001 61000000
16500001 17500000
191500001 - 192500000
9000001 10000000
78500001 79500000
27000001 28000000
5000001 6000000
76000001 77000000
7500001 8500000
182500001 - 183500000
70500001 71500000
102500001 - 103500000
89500001 90500000
6000001 7000000
102000001 - 103000000
207000001 - 208000000
96000001 97000000
149500001 - 150500000
49500001 50500000
192000001 - 193000000
231000001 - 232000000
88500001 89500000
160000001 - 161000000

Listing 8 : ...top 25...

Figure 10.1 was created quickly in R using ggplot2 to eyeball the difference in variant distribution across each
region (called a ’megabase’ due to their length of one million bases).. ...motivated by locating very highly
populated regions in the iCHIP group well above the mean in the output...

47

Fig. 10.1 Megabase Plot: ...

48

Chapter 11

Analysis Pipeline
Following the extraction of the Goldilocks region for each of the samples... we need to begin putting together
the pipeline that would process the work for the leave-one-out analysis...
...involving generation of indexes for each sample ...execute the pipeline and withold one of the Goldilocks
samples... ...using samtools to pileup the variants across all the samples (excluding one of course) and then
performing variant calling... ...must then compare the resulting VCF against the SNP VCF and calculate the
concordance to discover whether leaving out that particular sample has caused a change in the accuracy of the
variants called...

11.1

Implementation

11.1.1

Region Extraction

...regions extracted from IRODS ...difficulty gaining user access for both IRODS and "the farm"

11.1.2

samtools index

...index extracted regions

11.1.3

samtools mpileup

Following extraction of the Goldilocks region for each full-genome sample, the next stage is to use samtools
mpileup to ... ...each of the various full genome samples we have for processing. These files are then
summarised (and likelihoods are calculated) with samtools mpileup...

49

11.1 Implementation
samtools mpileup is a rather intensive process especially when in a scenario such as ours where there are
thousands of input files, one for each of the extracted regions. This not only places considerable strain on
the cluster’s file system but is incredibly inefficient given the overhead of file handling. As this pipeline will
need to be executed once for each leave-one-out experiment, it would be required to reduce the strain on the
file server before the job could be submitted. Usefully another member of the samtools collection provides
functionality to merge a list of given input files such as those extracted Goldilocks regions.
bsub -o ~/goldilocks/joblog/samtools_mpileup.%J.o -e
~/goldilocks/joblog/samtools_mpileup.%J.e -G hgi -J "samtools_mpileup"
-M1000 -R "select[mem>1000] rusage[mem=1000]" bash -c ’samtools mpileup -b
../goldilocks-3:46000001-47000000.fofn -g -I -f
/lustre/scratch113/resources/ref/Homo_sapiens/1000Genomes_hs37d5/hs37d5.fa >
~/goldilocks/all.withref.bcf’

11.1.4

samtools merge

Usage
...undocumented feature to use a file of filenames... allowing more than two files to be specified at a time...

Memory Leak
By default the LSF scheduler at the Sanger Institute will issue 100MB to any submitted job. Given both the
number of input files and their total size it was anticipated that this merge job would require more memory. At
the very least an estimated 35kB for a 64-bit pointer to each input file and approximately 150MB to house a
32kB buffer to read data from each file also. Yet executing the job with 500MB caused the LSF scheduler to
forceably terminate the process for exceeding the maximum allocated memory limit.
Assuming that the intermediate structures for storing and sorting the input data must have been greater than
expected, the job’s memory limit was generously increased with the syntax demonstrated by Listing 9 only to
meet the same fate, even when reserving 16GB of memory...
This is an absurd amount of memory even considering the vast input... It appeared a memory leak had been
discovered... that drastically increases in severity as more files are provided as input,
bsub -R"select[mem>4000] rusage[mem=4000]" -M4000 ...
#

|

|

| Raise maximum job memory to 4000mb

#

|

| Pre-reserve 4000mb for job before execution

#

| Only run on a node with more than 4000mb memory
Listing 9 : Flags required to raise job memory allocation

50

11.1 Implementation
...initial memory leak fixed by the author, several large variables not being freed from memory... ...following
this, further samtools merge jobs were submitted only to also be repeatedly terminated by the LSF scheduler
, this time for exceeding the maximum execution time limit for the queue...

Memory Leak in Test Harnesses
...getline ...regcomp...
...other memory leaks still remained in the tests and wanting to brush up on finding memory leaks with
valgrind, I volunteered to locate and patch these...
==30464== 416 bytes in 1 blocks are indirectly lost in loss record 85 of 103
==30464==
at 0x4A082F7: realloc (in /usr/lib64/valgrind/vgpreload_memcheck-amd64-linux.so)
==30464==
by 0x3FBCCCA725: duplicate_node (in /usr/lib64/libc-2.17.so)
==30464==
by 0x3FBCCD3ADA: duplicate_node_closure (in /usr/lib64/libc-2.17.so)
==30464==
by 0x3FBCCD415A: calc_eclosure_iter (in /usr/lib64/libc-2.17.so)
==30464==
by 0x3FBCCD79F6: re_compile_internal (in /usr/lib64/libc-2.17.so)
==30464==
at 0x4A08121: calloc (in /usr/lib64/valgrind/vgpreload_memcheck-amd64-linux.so)
==30464==
by 0x3FBCCCCD88: create_cd_newstate (in /usr/lib64/libc-2.17.so)
==30464==
by 0x3FBCCCD506: re_acquire_state_context.constprop.41 (in /usr/lib64/libc-2.17.so)
==30464==
by 0x3FBCC2132E: build_trtable (in /usr/lib64/libc-2.17.so)
==30464==
by 0x3FBCCD3792: re_search_internal (in /usr/lib64/libc-2.17.so)
==30464==
by 0x3FBCCD8E94: regexec@@GLIBC_2.3.4 (in /usr/lib64/libc-2.17.so)
==30464==
by 0x40C6FA: check_test_2 (test_trans_tbl_init.c:124)
==30464==
by 0x402CC4: main (test_trans_tbl_init.c:348)

Listing 10 : Example of valgrind locating a memory leak in one of the samtools test harnesses following the
failure to release memory allocated to a compiled regular expression

Poor Time Performance
...submitting the same job to the "long" (48hr) and "basement" (essentially unlimited) queues, it is clear that
the job is taking an extraordinary length of time to complete...
...during this time I took the opportunity to patch various memory leaks in the test harnesses of both merge
and split...
valgrind, the tool I used to track down memory leaks in the test harnesses of both samtools merge and samtools
split actually consists of more than just memcheck.
callgrind is a profiling tool that keeps track of a program’s call stack history, a handy feature built in to some
development environments such as QtCreator.
Having constructed a modest test set of files to merge, I called samtools merge from the command line,
attaching callgrind and later imported the resulting text file to QtCreator’s profiling tool to interpret the results
(I usually use KCacheGrind for this but I’ve been investigating QtCreator’s feature set for reasons unrelated

51

11.1 Implementation
to this project), the result is immediately obvious - millions of calls to functions in zlib; a free compression
library.
Further investigations using the QtCreator Analyze interface revealed that these calls all boiled down to one
line called not during the process of deflating the input files (as I had expected) but actually during the
compression of the output!

Fig. 11.1 callgrind output following merge with default output compression

Fig. 11.2 callgrind output following merge with uncompressed output
Looking at a brief explanation of the deflate algorithm, it seems reasonable to conclude the computational
cost is rather asymmetric between compressing and uncompressing - in that the effort is locating blocks to
compress and in comparison uncompressing is a reversible function on the known blocks.
Indeed, samtools merge specifies a -u option for uncompressed output and the callgrind output (second image)
indicates significantly less calls to zlib functionality.
It remains to be seen whether this option will cut down the time needed for the large merge job, perhaps this
is merely a red herring and we’re yet to discover the true speed trouble. In the meantime let’s see if sending
this job to the farm will work.

The Red Herring
... might be interesting to use gprof which is more geared towards finding functions that spend all your
execution time as opposed to callgrind which I believe counts CPU instructions.
After re-compiling htslib and samtools with the -pg flag to enable such profiling and executing the same
previous merge command on the modest test set, the output as parsed by gprof seems to indicate that the
trouble lies with bam_aux_get in htslib, with almost 50% of the execution time being spent in this particular
function.

uint8_t *bam_aux_get(const bam1_t *b, const char tag[2])
{
uint8_t *s;
int y = tag[0]<<8 | tag[1];
52

11.1 Implementation
s = bam_get_aux(b);
while (s < b->data + b->l_data) {
int x = (int)s[0]<<8 | s[1];
s += 2;
if (x == y) return s;
s = skip_aux(s);
}
return 0;
}
At a glance it seems that bam_aux_get receives a pointer to a BAM record and a “tag”, an array of two
characters representing an optional field as defined in Section 1.5 of the SAM file spec.
The function then appears to fetch all these auxiliary tags and iterates over each, comparing a transformation
of that tag (x) to a pre-computed transformation on the input tag (y).
This would of course be inherently slow for files with many such tags; especially given that the function is
called twice for potentially each line in a BAM file.

The Plot Thickens
...as we increase the number of input files, the time taken to read them in becomes non-linearly slower.
Currently my money is on the seemingly inefficient trans_tbl_init that appears to be called for each file, with
the current table of all previous files as an input...

11.1.5

bcftools call

...Unfortunately during the initial testing run of this step with all the Goldilocks regions it was discovered that
the output only included the standard header information and not a single line for the variants themselves.
...is because the piled up file was not generated with a corresponding reference DNA sequence and so the REF
(reference) column is set to N (an ambiguity code which translates to ‘any base’). bcftools call does have an
-M flag to prevent ignoring rows where the REF base is N (apparently called a "masked reference") however
this is currently causing a segmentation fault. Having recompiled htslib, samtools and bcftools I am now able
to run bcftools call on my local machine on some test data. I guess I’ll need to have someone recompile the
source for me on the cluster I’m using....

53

Part III

Discussions and Conclusions

54

Chapter 12

Current Status

55

Chapter 13

Critical Evaluation
...Section 3.2.3 The SciPy Stack also includes the "Python Data Analysis Library" (pandas), which is designed
to provide tools to supplement the Python environment, allowing users to perform analysis in Python instead
of having to switch to a more analysis focused language such as R.
...at first glance appears to be a far more developed Frontier... ...however it should be considered that Frontier
was designed to complement the machine learning experiments and provides
...whilst also providing the AbstractReader framework to allow users to quickly define methods to read in
their own data regardless of how esoteric or cryptic the file format is...

56

Chapter 14

Conclusions

57

[12] T. Therneau, B. Atkinson, and B. Ripley, rpart:
Recursive Partitioning, 2013, r package version
4.1-3. [Online]. Available: http://CRAN.R-project.
org/package=rpart
[13] R Core Team, R: A Language and Environment for
Statistical Computing, R Foundation for Statistical
Computing, Vienna, Austria, 2013. [Online].
Available: http://www.R-project.org/

References

[14] T. Smith. (2012–2013) aRrgh:
a newcomer’s
(angry) guide to R. [Online]. Available: http:
//tim-smith.us/arrgh/
[15] D. E. King, “Dlib-ml: A Machine Learning Toolkit,”
Journal of Machine Learning Research, vol. 10, pp.
1755–1758, 2009.

[1] T. Strachan and A. Read, Human Molecular Genetics,
4th ed. Garland Science, 2011, pp. 214–254.
A concise introduction to the processes
involved in massively parallel DNA
sequencing.
[2] M. Kircher, U. Stenzel and J. Kelso, “Improved
base calling for the Illumina Genome Analyzer
using machine learning strategies,” Genome Biology,
vol. 10, no. 8, p. R83, 2009.
Useful introduction to relevant Illumina
hardware and the errors that can occur
during sequencing.
[3] auto_qc: Additional high-throughput sequencing
autoQC steps [Github]. [Online]. Available:
https://github.com/wtsi-hgi/seq_autoqc
[4] vr-pipe: A generic pipeline system [Github]. [Online].
Available: https://github.com/wtsi-hgi/vr-pipe/

[16] C. Igel, V. Heidrich-Meisner, and T. Glasmachers,
“Shark,” Journal of Machine Learning Research,
vol. 9, pp. 993–996, 2008.
[17] Python Wiki. (2014) Python for Artificial Intelligence.
[18] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay, “Scikit-learn: Machine learning in
Python,” Journal of Machine Learning Research,
vol. 12, pp. 2825–2830, 2011.
A machine learning framework for
Python.

[6] Triage:
A todo manager for the disaster
that is your day [Github]. [Online]. Available:
https://github.com/SamStudio8/triage

ˇ Gorup, T. Hoˇcevar,
[19] J. Demšar, T. Curk, A. Erjavec, Crt
M. Milutinoviˇc, M. Možina, M. Polajnar, M. Toplak,
A. Stariˇc, M. Štajdohar, L. Umek, L. Žagar, J. Žbontar,
M. Žitnik, and B. Zupan, “Orange: Data Mining
Toolbox in Python,” Journal of Machine Learning
Research, vol. 14, pp. 2349–2353, 2013. [Online].
Available: http://jmlr.org/papers/v14/demsar13a.html

[7] sam/Dissertation | Triage. [Online]. Available:
https://triage.ironowl.io/sam/dissertation/

[20] scikit-learn. About Us. [Online]. Available: http:
//scikit-learn.org/stable/about.html

[8] Maggie Bartlett. (2011) Illumina HiSeq Flow
Cell. National Human Genome Research Institute.
Image. [Online]. Available: http://www.genome.gov/
dmd/img.cfm?node=Photos/Technology/Genome%
20analysis%20technology&id=80102

[21] T. E. Oliphant, “Python for Scientific Computing,”
Computing in Science & Engineering, vol. 9, no. 3,
pp. 10–20, 2007.

[5] M. Niemi, “Internal QC Summary Report,” 09 2012,
unpublished.

[9] H. Li, B. Handsaker, A. Wysoker, T. Fennell, J. Ruan,
N. Homer, G. Marth, G. Abecasis, R. Durbin, and
G. Subgroup, “ The Sequence Alignment/Map format
and SAMtools. ,” Bioinformatics, vol. 25, p. 2078,
2009.
[10] M. Hall, E. Frank, G. Holmes, B. Pfahringer,
P. Reutemann, and I. H. Witten, “The WEKA
Data Mining Software: An Update,” SIGKDD
Explorations Newsletter, vol. 11, no. 1, pp.
10–18, Nov. 2009. [Online]. Available:
http:
//doi.acm.org/10.1145/1656274.1656278
[11] B. Ripley, tree: Classification and regression trees,
2014, r package version 1.0-35. [Online]. Available:
http://CRAN.R-project.org/package=tree

[22] E. Jones, T. Oliphant, P. Peterson, et al., “SciPy: Open
source scientific tools for Python,” 2001–. [Online].
Available: http://www.scipy.org/
[23] SciPy.org. Scipy license. [Online].
http://scipy.org/scipylib/license.html

Available:

[24] NumPy.org. Numpy license. [Online]. Available:
http://www.numpy.org/license.html
[25] H. Wickham, ggplot2:
elegant graphics for
data analysis.
Springer New York, 2009.
[Online]. Available: http://had.co.nz/ggplot2/book
978-0-387-98140-6.
[26] S. Bethard. argparse 1.2.1: Python command-line
parsing library. [Online]. Available:
https:
//pypi.python.org/pypi/argparse/1.2.1

58

References
[27] M. Fowler, “Continuous Integration,” 2006.
[Online]. Available: http://www.martinfowler.com/
articles/continuousIntegration.html
[28] Kohsuke Kawaguchi. Meet jenkins. [Online].
Available:
https://wiki.jenkins-ci.org/display/
JENKINS/Meet+Jenkins
[29] Jenkins Usage Statistics. [Online]. Available:
http://stats.jenkins-ci.org/jenkins-stats/svg/svgs.html
[30] Getting
Started:
[Online]. Available:
getting-started/

Travis
CI
Overview.
http://docs.travis-ci.com/user/

[31] Introduction to wercker. [Online]. Available:
http://devcenter.wercker.com/articles/introduction/
[32] Hadley Wickham and Winston Chang, devtools:
Tools to make developing R code easier, 2013,
r package version 1.4.1. [Online]. Available:
http://CRAN.R-project.org/package=devtools
[33] Morandat, Floréal and Hill, Brandon and Osvald,
Leo and Vitek, Jan, “Evaluating the design of
the R language,” in ECOOP 2012–Object-Oriented
Programming. Springer, 2012, pp. 104–131.
[34] Python Software Foundation. More on Lists.
[Online]. Available:
https://docs.python.org/2/
tutorial/datastructures.html#more-on-lists
[35] ——. Dictionaries. [Online]. Available: https://docs.
python.org/2/tutorial/datastructures.html#dictionaries
[36] Python
Wiki.
(2012)
Time
Complexity.
[Online]. Available: https://wiki.python.org/moin/
TimeComplexity
[37] Python Software Foundation. Sets. [Online].
Available:
https://docs.python.org/2/tutorial/
datastructures.html#sets
[38] W. B. Van Rossum, Guido and N. Coghlan,
“PEP 8 – Style Guide for Python Code,” Python
Enhancement Proposals, 2001. [Online]. Available:
http://legacy.python.org/dev/peps/pep-0008/
[39] Kirk Strauser (http://stackoverflow.com/users/32538/kirk-strauser),
“Does
python
have
’private’
variables
in
classes?”
[Online].
Available:
http://stackoverflow.com/questions/1641219/
does-python-have-private-variables-in-classes
[40] Python Software Foundation. unittest — Unit
testing framework. [Online]. Available:
https:
//docs.python.org/2/library/unittest.html

59

Appendix A

Input Examples
A.1

BAMcheckR’d Example Output

# Summary
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN
SN

Numbers. Use ‘grep ^SN | cut -f 2-‘ to extract this part.
raw total sequences:
41400090
filtered sequences:
0
sequences:
41400090
is paired:
1
is sorted:
1
1st fragments:
20700045
last fragments:
20700045
reads mapped:
41291484
reads unmapped:
108606
reads unpaired:
60000
reads paired:
41231484
reads duplicated:
5756822
reads MQ0:
1038644
reads QC failed:
0
non-primary alignments:
0
total length:
3105006750
bases mapped:
3096861300
bases mapped (cigar):
3090885143
bases trimmed:
0
bases duplicated:
431761650
mismatches:
9107833
error rate:
0.002946675
average length:
75
maximum length:
75
average quality:
36
insert size average:
178.7
insert size standard deviation:
44.1
inward oriented pairs:
20577242
outward oriented pairs:
3140
pairs with other orientation:
3711
pairs on different chromosomes:
31535
fwd percent insertions above baseline:
1.43135383851191
fwd percent insertions below baseline:
0.686265539012562
fwd percent deletions above baseline:
1.38326380878871
fwd percent deletions below baseline:
0.44923551909251
rev percent insertions above baseline:
1.08264446659241
rev percent insertions below baseline:
0.457290262062496

60

A.1 BAMcheckR’d Example Output

SN
rev percent deletions above baseline:
1.15931214598243
SN
rev percent deletions below baseline:
0.413119424753248
SN
contiguous cycle dropoff count:
36
SN
fwd.percent.insertions.above.baseline:
1.43135383851191
SN
fwd.percent.insertions.below.baseline:
0.686265539012562
SN
fwd.percent.deletions.above.baseline:
1.38326380878871
SN
fwd.percent.deletions.below.baseline:
0.44923551909251
SN
rev.percent.insertions.above.baseline:
1.08264446659241
SN
rev.percent.insertions.below.baseline:
0.457290262062496
SN
rev.percent.deletions.above.baseline:
1.15931214598243
SN
rev.percent.deletions.below.baseline:
0.413119424753248
SN
quality.dropoff.fwd.high.iqr.start.read.cycle:
0
SN
quality.dropoff.fwd.high.iqr.end.read.cycle:
0
SN
quality.dropoff.fwd.high.iqr.max.contiguous.read.cycles:
0
SN
quality.dropoff.fwd.mean.runmed.decline.start.read.cycle:
20
SN
quality.dropoff.fwd.mean.runmed.decline.end.read.cycle:
51
SN
quality.dropoff.fwd.mean.runmed.decline.max.contiguous.read.cycles:
32
SN
quality.dropoff.fwd.mean.runmed.decline.high.value:
36.9775883578997
SN
quality.dropoff.fwd.mean.runmed.decline.low.value:
36.301749247405
SN
quality.dropoff.rev.high.iqr.start.read.cycle:
0
SN
quality.dropoff.rev.high.iqr.end.read.cycle:
0
SN
quality.dropoff.rev.high.iqr.max.contiguous.read.cycles:
0
SN
quality.dropoff.rev.mean.runmed.decline.start.read.cycle:
18
SN
quality.dropoff.rev.mean.runmed.decline.end.read.cycle:
56
SN
quality.dropoff.rev.mean.runmed.decline.max.contiguous.read.cycles:
39
SN
quality.dropoff.rev.mean.runmed.decline.high.value:
36.1517621338504
SN
quality.dropoff.rev.mean.runmed.decline.low.value:
35.3152133727245
SN
quality.dropoff.high.iqr.threshold:
10
SN
quality.dropoff.runmed.k:
25
SN
quality.dropoff.ignore.edge.cycles:
3
SN
A.percent.mean.above.baseline:
0.0991164444444441
SN
C.percent.mean.above.baseline:
0.127379555555556
SN
G.percent.mean.above.baseline:
0.0603679999999997
SN
T.percent.mean.above.baseline:
0.0868000000000005
SN
A.percent.mean.below.baseline:
0.0991164444444451
SN
C.percent.mean.below.baseline:
0.127379555555555
SN
G.percent.mean.below.baseline:
0.0603680000000002
SN
T.percent.mean.below.baseline:
0.0867999999999993
SN
A.percent.max.above.baseline:
0.601733333333332
SN
C.percent.max.above.baseline:
0.394266666666667
SN
G.percent.max.above.baseline:
0.2956
SN
T.percent.max.above.baseline:
0.768000000000001
SN
A.percent.max.below.baseline:
0.318266666666666
SN
C.percent.max.below.baseline:
0.825733333333332
SN
G.percent.max.below.baseline:
0.554400000000001
SN
T.percent.max.below.baseline:
0.251999999999999
SN
A.percent.max.baseline.deviation:
0.601733333333332
SN
C.percent.max.baseline.deviation:
0.825733333333332
SN
G.percent.max.baseline.deviation:
0.554400000000001
SN
T.percent.max.baseline.deviation:
0.768000000000001
SN
A.percent.total.mean.baseline.deviation:
0.198232888888889
SN
C.percent.total.mean.baseline.deviation:
0.254759111111111
SN
G.percent.total.mean.baseline.deviation:
0.120736
SN
T.percent.total.mean.baseline.deviation:
0.1736
# First Fragment Qualitites. Use ‘grep ^FFQ | cut -f 2-‘ to extract this part.
# Columns correspond to qualities and rows to cycles. First column is the cycle number.
FFQ
1
8968
3619
9863
747
5094
0
6642
FFQ
2
21676
0
0
0
0
0
0
0
43
FFQ
3
7
0
177
0
0
0
0
0
0
[...]
FFQ
74
3697
39
0
919
4933
0
0
56866
FFQ
75
4542
0
0
0
0
0
4634
77822
FFQ
76
0
0
0
0
0
0
0
0
0

61

1609

4673
1885

0

...
1524

0
...

...
...

...
...

A.1 BAMcheckR’d Example Output

# Last Fragment Qualitites. Use ‘grep ^LFQ | cut -f 2-‘ to extract this part.
# Columns correspond to qualities and rows to cycles. First column is the cycle number.
LFQ
1
8869
0
0
0
0
0
63
0
0
1156
...
LFQ
2
3300
0
0
0
0
0
0
0
0
0
...
LFQ
3
6816
0
0
0
573
0
83
0
7011
...
[...]
LFQ
74
5980
3
91
0
0
0
1340
9696
72939
...
LFQ
75
4314
0
0
168
0
848
8591
0
70358
...
LFQ
76
0
0
0
0
0
0
0
0
0
0
...
# Mismatches per cycle and quality. Use ‘grep ^MPC | cut -f 2-‘ to extract this part.
# Columns correspond to qualities, rows to cycles. First column is the cycle number, second
# is the number of N’s and the rest is the number of mismatches
MPC
1
14078
0
2594
6777
416
1919
0
2222
...
MPC
2
21407
0
0
0
0
0
0
0
0
5
...
MPC
3
3205
0
0
37
0
43
0
12
0
...
[...]
MPC
74
779
0
3
0
131
440
0
93
7485
...
MPC
75
136
0
0
0
3
0
47
704
9302
...
MPC
76
0
0
0
0
0
0
0
0
0
...
# GC Content of first fragments. Use ‘grep ^GCF | cut -f 2-‘ to extract this part.
GCF
0.5
56
GCF
1.76
60
GCF
3.02
126
GCF
4.27
212
GCF
5.78
347
[...]
GCF
93.72
378
GCF
95.23
186
GCF
96.48
87
GCF
97.74
55
GCF
99.25
17
# GC Content of last fragments. Use ‘grep ^GCL | cut -f 2-‘ to extract this part.
GCL
0.5
118
GCL
1.76
175
GCL
3.02
230
GCL
4.27
354
GCL
5.78
525
[...]
GCL
93.72
613
GCL
95.23
430
GCL
96.48
274
GCL
97.74
185
GCL
99.25
110
# ACGT content per cycle. Use ‘grep ^GCC | cut -f 2-‘ to extract this part. The columns are: cycle, and A,C,G,T counts [%]
GCC
1
26.93
23.09
22.77
27.2
GCC
2
26.78
23.24
22.97
27.02
GCC
3
26.46
23.59
23.3
26.66
GCC
4
26.29
23.79
23.45
26.46
GCC
5
26.47
23.61
23.3
26.62
[...]
GCC
70
26.09
24.26
23.45
26.2
GCC
71
26.07
24.25
23.46
26.22
GCC
72
26.04
24.27
23.49
26.2
GCC
73
26.07
24.25
23.47
26.22
GCC
74
26.08
24.24
23.45
26.23
GCC
75
26.01
24.31
23.51
26.18
# Insert sizes. Use ‘grep ^IS | cut -f 2-‘ to extract this part.
# The columns are: pairs total, inward oriented pairs, outward oriented pairs, other pairs
IS
0
10
0
1
9
IS
1
3
0
3
0
IS
2
4
0
4
0
IS
3
5
0
5
0
IS
4
2
0
2
0

62

A.1 BAMcheckR’d Example Output

IS
5
3
0
3
0
[...]
IS
110
33952
33952
0
0
IS
111
38433
38433
0
0
IS
112
43373
43370
0
3
IS
113
48160
48159
0
1
IS
114
53175
53171
0
4
IS
115
59504
59502
0
2
IS
116
64668
64668
0
0
IS
117
71107
71105
0
2
IS
118
77157
77156
0
1
IS
119
84044
84044
0
0
IS
120
90116
90110
3
3
[...]
IS
327
6546
6546
0
0
IS
328
6483
6483
0
0
IS
329
6201
6201
0
0
IS
330
6228
6228
0
0
IS
331
5852
5852
0
0
# Read lengths. Use ‘grep ^RL | cut -f 2-‘ to extract this part. The columns are: read length, count
RL
75
41400090
# Indel distribution. Use ‘grep ^ID | cut -f 2-‘ to extract this part.
# The columns are: length, number of insertions, number of deletions
ID
1
128650
183418
ID
2
26409
39770
ID
3
10213
16046
ID
4
7756
11444
ID
5
1746
3455
[...]
ID
35
0
8
ID
36
0
1
ID
37
0
1
ID
38
0
1
ID
40
0
2
# Indels per cycle. Use ‘grep ^IC | cut -f 2-‘ to extract this part.
# The columns are: cycle, number of insertions (fwd), .. (rev) , number of deletions (fwd), .. (rev)
IC
1
0
0
105
97
IC
2
24
15
150
179
IC
3
129
138
441
509
IC
4
253
310
623
829
IC
5
557
724
786
1164
[...]
IC
70
571
710
638
761
IC
71
350
428
309
434
IC
72
154
150
38
45
IC
73
60
61
15
23
IC
74
20
19
11
12
# Coverage distribution. Use ‘grep ^COV | cut -f 2-‘ to extract this part.
COV
[1-1]
1
332980694
COV
[2-2]
2
105004580
COV
[3-3]
3
29112182
COV
[4-4]
4
13415014
COV
[5-5]
5
6716815
[...]
COV
[996-996]
996
2
COV
[997-997]
997
2
COV
[998-998]
998
2
COV
[1000-1000]
1000
4
COV
[1000<]
1000
116
# GC-depth. Use ‘grep ^GCD | cut -f 2-‘ to extract this part.
# The columns are: GC%, unique sequence percentiles, 10th, 25th, 50th, 75th and 90th depth percentile
GCD
0
0.001
0
0
0
0
0
GCD
0.4
0.002
0.101
0.101
0.101
0.101
0.101

63

A.2 auto_qc Decision Matrix

GCD
GCD
GCD
[...]
GCD
GCD
GCD
GCD
GCD

A.2
lanelet
9999_9#1
9999_9#3
9999_9#2
9999_9#5
9999_9#4
9999_9#7
9999_9#6
9999_9#9
9999_9#8

A.3
# gwas
cd-gwas
uc-gwas
# ichip
cd-ichip
uc-ichip

19
20
21

0.003
0.004
0.004

0.049
0.06
0.045

0.049
0.06
0.045

0.049
0.06
0.045

66
67
68
69
72

99.99
99.994
99.997
99.999
100

0.244
1.279
4.148
0.499
0.476

2.693
1.279
4.148
0.499
0.476

6.746
4.305
4.463
0.499
0.476

0.049
0.06
0.045

0.049
0.06

11.794
9.667
5.741
1.935
1.219

auto_qc Decision Matrix
sample
study
AQCTest000000
AQCTest000002
AQCTest000001
AQCTest000004
AQCTest000003
AQCTest000006
AQCTest000005
AQCTest000008
AQCTest000007

npg
AQCTest
AQCTest
AQCTest
AQCTest
AQCTest
AQCTest
AQCTest
AQCTest
AQCTest

aqc
pass
fail
pass
warn
warn
fail
pass
warn
pass

...
passed
failed
pass
warn
warning
fail
passed
warning
passed

Goldilocks Pathsfile
/pools/encrypted/sanger/vcf/cd-seq.vcf.gz.q
/pools/encrypted/sanger/vcf/uc-seq.vcf.gz.q
/pools/encrypted/sanger/vcf/cd.ichip.vcf.gz.q
/pools/encrypted/sanger/vcf/uc.ichip.vcf.gz.q

64

...
...
...
...
...
...
...
...
...

0.045
15.885
11.483
7.354
1.935
1.219

Appendix B

Output Examples
B.1

Frontier Log Example

Frontier
********
Data Dir
AQC File

/pools/encypted/sanger/frontier/data/bamcheck_2013dec25/uc
/pools/encypted/sanger/frontier/data/crohns-uc-table-a.2013dec25.manual_qc_update.txt

Class Def
Class Read
Class Used

fail
1542
234

Total Read
Total Used

3947
3371

Param Set
Param Count
Param List:

pass
9154
3137

warn
2759
576

ALL
82
1st-fragments
A-percent-max-above-baseline
A-percent-max-baseline-deviation
A-percent-max-below-baseline
A-percent-mean-above-baseline
A-percent-mean-below-baseline
A-percent-total-mean-baseline-deviation
C-percent-max-above-baseline
C-percent-max-baseline-deviation
C-percent-max-below-baseline
C-percent-mean-above-baseline
C-percent-mean-below-baseline
C-percent-total-mean-baseline-deviation
G-percent-max-above-baseline
G-percent-max-baseline-deviation
G-percent-max-below-baseline
G-percent-mean-above-baseline
G-percent-mean-below-baseline
G-percent-total-mean-baseline-deviation
T-percent-max-above-baseline
T-percent-max-baseline-deviation
T-percent-max-below-baseline
T-percent-mean-above-baseline

65

B.1 Frontier Log Example

T-percent-mean-below-baseline
T-percent-total-mean-baseline-deviation
average-length
average-quality
bases-duplicated
bases-mapped
bases-mapped-(cigar)
bases-trimmed
error-rate
filtered-sequences
fwd-percent-deletions-above-baseline
fwd-percent-deletions-below-baseline
fwd-percent-insertions-above-baseline
fwd-percent-insertions-below-baseline
insert-size-average
insert-size-standard-deviation
inward-oriented-pairs
is-paired
is-sorted
last-fragments
maximum-length
mismatches
non-primary-alignments
outward-oriented-pairs
pairs-on-different-chromosomes
pairs-with-other-orientation
quality-dropoff-fwd-high-iqr-end-read-cycle
quality-dropoff-fwd-high-iqr-max-contiguous-read-cycles
quality-dropoff-fwd-high-iqr-start-read-cycle
quality-dropoff-fwd-mean-runmed-decline-end-read-cycle
quality-dropoff-fwd-mean-runmed-decline-high-value
quality-dropoff-fwd-mean-runmed-decline-low-value
quality-dropoff-fwd-mean-runmed-decline-max-contiguous-read-cycles
quality-dropoff-fwd-mean-runmed-decline-start-read-cycle
quality-dropoff-high-iqr-threshold
quality-dropoff-ignore-edge-cycles
quality-dropoff-rev-high-iqr-end-read-cycle
quality-dropoff-rev-high-iqr-max-contiguous-read-cycles
quality-dropoff-rev-high-iqr-start-read-cycle
quality-dropoff-rev-mean-runmed-decline-end-read-cycle
quality-dropoff-rev-mean-runmed-decline-high-value
quality-dropoff-rev-mean-runmed-decline-low-value
quality-dropoff-rev-mean-runmed-decline-max-contiguous-read-cycles
quality-dropoff-rev-mean-runmed-decline-start-read-cycle
quality-dropoff-runmed-k
raw-total-sequences
reads-MQ0
reads-QC-failed
reads-duplicated
reads-mapped
reads-paired
reads-unmapped
reads-unpaired
rev-percent-deletions-above-baseline
rev-percent-deletions-below-baseline
rev-percent-insertions-above-baseline
rev-percent-insertions-below-baseline
sequences
total-length
Feature Importances:
0.32
0.20

T-percent-mean-above-baseline
T-percent-mean-below-baseline

66

B.1 Frontier Log Example

0.14
0.14
0.05
0.05
0.03
0.02
0.01
CV Score (Fld)
Decision PDF
********

rev-percent-insertions-above-baseline
T-percent-total-mean-baseline-deviation
bases-duplicated
fwd-percent-insertions-above-baseline
reads-duplicated
insert-size-average
insert-size-standard-deviation
1.00 +/- 0.01 (10)

pdf/2014-03-23_2151__IGNWARN_ALL_/

67

Appendix C

Tool Installation
C.1

htslib

# Download repositories from Github
git clone htslib
cd htslib/
make

C.2

bcftools

Prerequisites: htslib(Appendix C.1)
# Download repositories from Github
git clone bcftools
cd bcftools/
# Build bcftools
make #(requires htslib to be above samtools/bcftools dir)
sudo cp bcftools usr/local/bin

68

C.3 samtools

C.3

samtools

Prerequisites: htslib(Appendix C.1)
# Download repositories from Github
git clone samtools
cd samtools/
# Build samtools
make #(requires htslib to be above samtools/samtools dir)
sudo cp samtools usr/local/bin

C.4

tabix

# Tabix
# Download from sourceforge
# http://sourceforge.net/projects/samtools/files/tabix/
make
sudo cp tabix /usr/local/bin
# Generate an index (vcfidx) (not necessary)
vcftools --gzvcf cd.ichip.vcf.gz
# Index with tabix
# "The input data file must be position sorted and compressed by bgzip
# which has a gzip(1) like interface"
tabix -p vcf file.vcf.gz
diff cd-seq.vcf.gz.tbi diff cd-seq.vcf.gz.tbi.sanger
# Shows no difference :)
# http://samtools.sourceforge.net/tabix.shtml

69

