%******************************************************************************
\chapter{Introduction}
\ifpdf
    \graphicspath{{Chapter1/Figs/Raster/}{Chapter1/Figs/PDF/}{Chapter1/Figs/}}
\else
    \graphicspath{{Chapter1/Figs/Vector/}{Chapter1/Figs/}}
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Over the past few years advances in genetic sequencing hardware have introduced
the concept of massively parallel DNA sequencing; allowing potentially billions
of chemical reactions to occur simultaneously, reducing both time and cost
required to perform genetic analysis\citep{HMG}. However, these
"next-generation" processes are complex and open to error\citep{Illumina}, thus
quality control is an essential step to assure confidence in any downstream
analyses performed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Project Aims}

The project consists of two sub-projects;

\begin{itemize}
    \item Analysis of a current quality control system in place
    \item Identification of quantifiable sample properties that affect downstream analysis
\end{itemize}


%TODO Samples or lanelets?
%TODO Github link in footnote rather than reference?
\subsection{Analysis of Current System}
% Lanes and lanelets have not been introduced yet and I think this works better
% as an introduction to the project rather than getting too heavy with
% terminology first.
With the support of the Wellcome Trust Sanger Institute in Cambridge, this
project works with the Human Genetics Informatics team to investigate
\textbf{auto\_qc}\citep{github:autoqc}, the institute's current automated quality control tool.

During genetic sequencing a large number of metrics are generated to determine
the quality of the data read from the sequencing hardware itself. As part of the
current vertebrate sequencing pipeline\citep{github:vr-pipe} at the institute,
\textbf{auto\_qc} is responsible for applying quality control to samples within
the pipeline by comparing a modest subset of these metrics to simple hard-coded
hard thresholds; determining whether a particular sample has reached a level that
requires a warning, or has exceeded the threshold and failed entirely. Whilst
this does catch most of the very poor quality outputs, a large number of samples
are flagged for manual inspection at the warning level; a time consuming task
which invites both inefficiency and error.

In practise most of these manual decisions are based on inspecting a range of
diagnostic plots which suggests that a machine learning classifier could
potentially be trained on the combinations of quality control statistics
available to make these conclusions without the need for much human
intervention\citep{marireport}.

The first part of the project aims to apply machine learning techniques to
replicate the current \textbf{auto\_qc} rule set by training a decision tree
classifier on a large set of these quality metrics. The idea is to investigate
whether these simple threshold based rules can be recovered from such data, or
whether a new classifier would produce different rules entirely. During this
analysis it is hoped the classifier may be able to identify currently unused
quality metrics that improve labelling accuracy. An investigation on the
possibility of aggregating or otherwise reducing the dimensions of some of the
more detailed quality statistics to create new parameters will also be
conducted.

The goal is to improve efficiency of quality control classification, whether by
improving accuracy of pass and fail predictions over the current system or
merely being able to provide additional information to a lab technician
inspecting samples labelled with a warning to reduce arbitrary decisions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Identification of Properties that affect Downstream Analysis}

The other half of this project is motivated by the question "What \textit{is}
good and bad in terms of quality?"

To be able to classify samples as a pass or a fail with understanding, we need
an idea of what actually constitutes a good or bad quality sample and must look
at the effects quality has on analysis performed downstream from sequencing.
An example of such is \textbf{variant calling} --- the process of identifying
differences between a DNA sample (such as your own) and a known reference
sequence.

%TODO Need to improve explanation here, don't like the simplification and feel
% it might just be hindering understanding of what we're doing
%TODO Difficult to discuss this without the word lanelets, samples vs.
% sub-samples isn't making as much sense as I'd like
%TODO Use a figure
Given two high quality data sources where DNA sequences from individuals were
identified in two different ways (one of which being next-generation sequencing)
it would be possible to measure the difference between each corresponding pair.
Using this, we could investigate the effect of leaving out part of the
next-generation sample during the variant calling process.
If we were to leave a part of a sample out of the variant calling
pipeline would the variants found be more (or less) accurate than if it had been
included? Would they agree more (or less) with the variants called after using
the non next-generation sequencing method?

Having identified such sub-samples, can quality control metrics from the
previous part be found in common? If so, such parameters would identify "good"
or "bad" samples straight out of the machine! Samples that exhibit these quality
variables will go on to improve or detriment analysis.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Project Method}
\subsection{Methodology}
\label{chap:methodology}

%TODO More coherant waffling needed
%TODO Explain CRACK customer?
%TODO Do I need to reference this section as my Assignment 3 report?
Clearly some team-based practices invited by agile methodologies ---
pair programming immediately comes to mind --- are not applicable in a solo
project. It is also unreasonable to expect an "on-site" customer for this
particular project. In \textit{The Case Against Extreme Programming}, Matt Stephens
describes a "self referential safety net" where the perceived traps in each
practice are supported and "made safe" by other XP practices.  This would seem
to rule out XP as a viable methodology for a solo-project as cutting out some
of the processes that allow this form of evolutionary design to work (and
flatten that cost-of-change curve) can introduce serious flaws to the
management of a project and potentially result in failure. In the same breath
it is important to remember that not all agile processes need be discarded just
because XP seems incompatible. Indeed, some processes seem like common sense,
for example; frequent refactoring, simple design, continuous integration and
version control. Test driven development could also prove a useful process to
consider as part of a methodology for this project, setting up a framework that
allows for quick and frequent testing (before coding) and ensuring that any
refactoring has a positive (or at least non-negative) effect on the system
could be a worthwhile contribution to efficiency.

Could a more plan driven approach or form of agile-plan hybrid be considered
appropriate here? In \textit{Balancing Agility and Discipline}, Boehm
and Turner introduce the idea of "homegrounds" for both agile and plan driven
approaches; I note here that for projects that require high reliability and
feature a non-collocated "CRACK customer" in fact align with some of these
homegrounds for plan driven development. Combined with the thought that the
project requirements will also be relatively stable it would seem that there
may be no reason to switch to a more agile methodology as its primary feature
is the welcoming of change that is not even needed? Perhaps this is the naivety
of an optimist!

%TODO Using I
%TODO Ring of Snakes probably needs elaboration
Personally I think I would approach this with a form of agile-plan hybrid; I
like the idea of quick iterations and getting feedback as opposed to leaving
acceptance testing until the very end of the project, but I also want a somewhat
detailed feedback process. Whilst through Neil Taylor's course \textit{Agile
Methodologies} it was suggested that it is dangerous to pick and choose
processes (donâ€™t anger the Ring of Snakes!) and also merely paying "lip service"
to agile must be avoided (otherwise what's the point?), I feel that on this
occasion it can be justified by the size of the project itself.

%TODO Opportunity to drop the word pipeline and describe what it is for future
%use
This project will consist of many research steps, each requiring some form of
computational process to prepare the data for the next step. Whilst the
implementations of the algorithms themselves pose computational complexity,
there appears to be little challenge from a planning perspective and in fact a
looser overall plan should probably be considered as we must account for
unforeseen and unexpected outcomes from each research step.

The most important part of ensuring this project stays on track will be the
development of a sensible testing methodology to ensure we are not only moving
in the right direction in terms of which algorithm and parameters to use but
also in terms of reliably measuring performance over time in a way that allows
justification of such design choices.

Despite this trail of thought, given the research grounding this project entails
it might be required to look beyond traditional and even modern software
development methodologies and investigate a more scientific approach. A simple
"scientific method" would involve establishing a null hypothesis that can be
proven false by testing (eg: "\textbf{auto\_qc} classifier is more
accurate than the new classifier") and executing experiments that
attempt to prove this null hypothesis false in favour of an alternative
hypothesis (typically the opposite, eg: "The new classifier is more
accurate than the old classifier"). This form of hypothesis testing
could essentially become the project's acceptance tests (providing we have an
empirical definition of what "more accurate" means in terms of this
system) and any modification can be classed as an experiment ("Do these
parameters allow us to reject the null hypothesis?").  Although care must be
taken not to let this descend into unstructured cycles of mere hack-and-test,
code-and-fix style programming.

Overall it is rather difficult to select a methodology for a project such as
this, the research element makes it almost impossible to draw on previous
personal experience for ideas of what development processes may or may not be
effective.

\subsection{Task Management}
Shortly before embarking on this dissertation project, I had written my own todo
list web application; \textbf{Triage}\citep{github:triage}. This will be useful
in keeping track of current objectives and allow prioritisation. The list used to
organise my project is also publicly available\citep{sam:triage}.

\subsection{Time Considerations} It must be remembered that this project needs
to meet the requirements for a \textit{minor} project and is to be completed
alongside the study of several other modules which each have their own
assignments and obligations. It would be easy to become overly ambitious and
thus aims and goals will need to be revised as both obstacles and breakthroughs
are encountered.
