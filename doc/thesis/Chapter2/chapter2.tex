%*****************************************************************************************
\part{Analysis of Current System}
\chapter{Introduction and Background}
\label{chap:autoqc}

\ifpdf
    \graphicspath{{Chapter2/Figs/Raster/}{Chapter2/Figs/PDF/}{Chapter2/Figs/}}
\else
    \graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

This part of the project can be outlined as follows:

\begin{itemize}
    \item Collect data sets on which a machine learning classifier is to be trained
    \item Construct a program capable of processing and storing such data sets
        such that required subsets of the data can be quickly and easily
        returned for further analysis
    \item Select a suitable machine learning framework to handle the training
        and validation of a classifier
    \item Ensure a robust validation methodology exists for assuring quality of
        our own results
    \item Set up an environment capable of allowing results from such a
        classifier to be stored and compared
    \item Training a suitable classifier on the collected data sets
    \item Perform experiments by selecting subsets of the variables and
        observations and measure whether classification accuracy is improved
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Concepts and Terminology}
See Appendix~\ref{app:concepts-p0} for an introduction to terminology used
throughout Part I including; next-generation sequencing, samples, lanes and
lanelets.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Materials and Methods}
\section{Input Data and Format}
\subsection{"BAMcheckR'd" Data}
\label{chap:bamcheckr-data}

%NOTE Assuming lanelets have been described by this point...
%TODO Cite and explain the security policy for this data?
As part of the project I have been granted access to significant data sets at the
Sanger Institute, unlocking quality control data for two of the largest studies
currently undergoing analysis. A wide array of quality metrics are available for
each and every lanelet that forms part of either of the two studies, totalling
13,455 files: 9154 (68\%) passes, 1542 (11\%) fails and 2759 (21\%) warnings.

The files are created by \textbf{samtools stats} -- part of a collection of
widely used open-source utilities for post-processing and manipulation of large
alignments such as those produced by next-generation sequencers that are
released under the umbrella name of "SAMtools"\citep{samtools} (Sequence
Alignment and Map Tools). \textbf{samtools stats} collects statistics from
sequence data files and produces key-value summary numbers as well as more
complex tab delimited dataframes tabulating several metrics over time.

The output of \textbf{samtools stats} is then parsed by an in-house tool called
\textbf{bamcheckr}\footnote{Named such as \textbf{samtools stats} now incorporates
\textbf{bamcheck} and the tool is written in R} which supplements the summary
numbers section of the \textbf{samtools stats} output with additional metrics
that are later used by \textbf{auto\_qc} for classification.  This process
appends additional key-value pairs in the summary numbers section.  A truncated
example of a "bamcheckr'd" file can be found in Appendix~\ref{app:bamcheckr}.

It is these summary numbers that will be the main focus of our learning task.


\subsection{auto\_qc Decision Data}

To use these "bamcheckr'd" files for training and testing a machine learning
classifier, it is necessary to map each file to a classification result from
\textbf{auto\_qc}. The one-to-one mapping between each input file and its label
are provided by the Sanger Institute in a separate file hereafter referred to as
the \textit{AQC Decision Matrix} or \textit{AQC (Decision) File}.

A truncated example of such a file can be found in
Appendix~\ref{app:aqc_matrix}.  Only the first few columns are included --
indeed we are only interested in the \textit{lanelet} and \textit{aqc} fields
which provide an identifier that maps the row to a given input file and its
classification by \textbf{auto\_qc} respectively.  Latter columns pertain to a
breakdown of decisions made by \textbf{auto\_qc} which are not included in the
example for confidentiality (and brevity).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Development Environment}
\label{part1:dev}

\subsection{Language}
\label{part1:dev:lang}

Python was selected for the language of the program designed to handle this vast
array of input data, more out of personal taste rather than a detailed analysis
of required performance and features. From previous experience I was happy with
the performance of Python when processing large datasets in terms of both
file handling operations and storing the data in memory for later use. Python's
generous choice of both built-in and third-party libraries have proven useful.
Due to its concise and flexible nature it is possible to rapidly
develop applications and its readability eases ongoing maintenance; useful given
the short time-span allocated for this project and the possibility of others
wishing to contribute to the project codebase after completion.

Whilst the choice was made primarily on preference, this is not to say other
options were not considered: a highly popular Java-based collection of data
mining tools, \textbf{WEKA}\citep{weka} would certainly have provided a
framework for building decision tree classifiers but did not
appear to offer any significant features that were unavailable elsewhere, whilst
Java itself has the added constraint of requiring a virtual machine to be
installed which could be undesirable from a performance or security
standpoint when the application is deployed to servers at the Sanger Institute.

Difficulty was also encountered finding example implementations for \textbf{WEKA}
with most documentation and tutorials providing information for performing
analysis via the graphical "Explorer" interface instead, which would not be
appropriate for quickly setting up and repeating experiments automatically.

Given the quality data we'll be using to train a machine learning classifier is
output from the previously mentioned R script, \textbf{bamcheckr}, it was worth
briefly investigating the options available for R itself as the potential of
integrating the learning and predicting functions right into the same process
that outputs the data seemed convenient.

Whilst the \textbf{tree}\citep{man:rtree} and \textbf{rpart}\citep{man:rpart}
packages are available for constructing decision trees in R (and actually
\textbf{RWeka} provides an R interface to \textbf{WEKA}) neither appeared to be
as robust as other more well-known frameworks. Also putting it politely, the
programming paradigm of R\citep{man:R} is rather different to other languages
and can significantly increase development time (and frustration\citep{argh}) if
one is not well versed in the patterns and grammar of the language  and it
seemed best to stick to one's comfort zone given the brief timescale for the
project.

Had performance been a critical decision factor, lower level languages such as
C, C++ or even Fortran could be used. Briefly looking at frameworks available
for C++ in particular, two popular solutions include: \textbf{dlib}\citep{dlib},
which did not support tree-based classifiers but did offer implementations of
other potentially useful algorithms such as multi-class support vector machines;
and \textbf{Shark}\citep{shark}, which supported both decision tree and random
forest classifiers. Both packages also provided a series of utility functions
for performing mathematical or statistical operations on vectors of data.


\subsection{Framework}

Having studied the \textit{Machine and Intelligent Learning} module in my final
year, the prospect of getting stuck into the deep of a machine learning
algorithm was exciting. However the reality is a lot of cumulative time and
effort has gone into the creation and optimisation of a framework, which is
unlikely to be surpassed successfully by a short-term one-person project. Thus
utilisation of a third party machine learning library seems a wise investment
for the project's codebase.

There are clearly numerous machine learning frameworks available in many
languages, some of which were touched upon in the previous section and formed
part of the development environment decisions.
Whilst it is obviously unnecessary to select a framework which uses
the same language as the project, it seemed counter-intuitive to select otherwise,
for the establishing of additional arbitrary output and input steps to move data
between the two environments could impede prompt experiment repeatability and
introduce error.

A mixed bag of machine learning frameworks exist in Python\citep{py:for-ai},
ranging from several highly active general purpose libraries to a multitude of
smaller projects that focus on one specific learning task.

I investigate two of the larger libraries;
\textbf{scikit-learn}\citep{scikit-learn} and \textbf{Orange}\citep{orange},
primarily as their general purpose nature will allow exploration of various
classifier solutions (given time) without the need to integrate many packages
together, but also for their built-in functionality that aids the measuring of
classifier performance and accuracy. These two packages were also selected for
investigation on their recommendation from the project supervisor.


\textbf{scikit-learn} is somewhat the "new kid on the block" in terms of machine
learning packages for Python, originally starting as a Google Summer of Code
project in 2007 under the name of \textbf{scikits.learn}\footnote{Named as a
\textbf{SciPy} Toolkit for Learning Tasks} the package was not officially
published until 2010 where it quickly gained popularity and built an
international team of contributors\citep{about-scikit-learn}.

Arguably one of the most useful features of \textbf{scikit-learn} is its
seamless integration with the "big names" of scientific computing in Python:
\textbf{NumPy}\citep{NumPySciPy} and \textbf{SciPy}\citep{SciPy}, making heavy
use of the optimised data structures and mathematical algorithms these packages
offer, not only providing improved performance against packages that implement
their own structures but also easing interaction by not requiring users to
populate abstruse structures with their data or implement such algorithms
themselves.

\textbf{scikit-learn} offers support for a wide range of algorithms covering
many different machine learning tasks, including decision trees and random
forests. Also of interest are the various utility subpackages, taking particular
note of one for measuring the performance of the various classifiers;
including functions for executing cross-validation and generating confusion
matrices; and another which assists with the selection of parameters (features)
to use in a machine learning model.

The library boasts a growing community of users and a dedicated team of
developers\footnote{Some full time developer positions are funded by
\textit{INRIA}, the French Institute for Research in Computer Science and
Automation} who submit hundreds of commits to the project repository each month
according to Github Pulse.


\textbf{Orange}, like \textbf{scikit-learn} is designed to be a general purpose
machine learning framework and provide a wide array of tools to work with many
different types of problems. Setting it apart from many of the other packages is
the inclusion of a graphical user interface (GUI) that presents
drag-and-drop-esque access to widgets from which users assemble a workflow to
apply to their data. The GUI also allows visualisation of elements such as
decision trees or confusion matrix plots without any effort from the end user.

At first glance \textbf{Orange} appears to offer a larger collection of
functions to work specifically with classification tasks when compared to
\textbf{scikit-learn}. Indeed it would later be discovered that it ships with
features useful to the project that have not been implemented in
\textbf{scikit-learn} including tree pruning and printing.

However it does not feature the same integration with
Python's scientific computing packages; \textbf{NumPy} and \textbf{SciPy} and
also requires that end users input their data in a structure defined by
the framework rather than a generic data structure.
Thus use of \textbf{Orange} would introduce more data wrangling and munging
steps to ensure that data is represented in the correct format for a classifier
to perform its learning and prediction role and also to interpret returned objects.

Although \textbf{Orange} affords a suitable (and in some cases better) range of
functions to use for decision trees, I was concerned at the push to use its GUI
as opposed to the package API, potentially impeding experiment repeatability by
having to reset the graphical environment each time.

Despite the \textbf{scikit-learn} package not yet reaching a major version
milestone, its feature set, interaction with \textbf{NumPy} and \textbf{SciPy}
and useful built-in subpackages led me to choose \textbf{scikit-learn} as the
machine learning framework for this part of the project.


\subsection{Additional External Libraries}
\label{sec:additional-libs}

As discussed, the project will make use of the open source\footnote{Components
of the \textbf{SciPy Stack} are distributed under the 3-clause Modified BSD
License\citep{scipy-lic}\citep{numpy-lic}} \textbf{SciPy Stack}, consisting of
several core packages including \textbf{NumPy}\citep{NumPySciPy}: a package for
efficient numerical computation which defines generic N-dimensional array and
matrix data containers; and \textbf{SciPy}\citep{SciPy}, which provides a
multitude of optimised numerical routines.

Whilst the stack also includes \textbf{Matplotlib}, a highly popular graphing
package, I typically prefer to use the R package \textbf{ggplot2}\citep{ggplot2}
for creating graphs. However it is useful to note that \textbf{Matplotlib}
integrates easily with the data structures of \textbf{NumPy} (and thus
\textbf{scikit-learn} too).

\textbf{argparse} is a third-party Python library\footnote{Although since Python
2.7, \textbf{argparse} has been included as part of the standard
library\citep{argparse-pypi}} used for specifying the arguments and options of a
command line interface via a simple API.  \textbf{argparse} spares the developer
from having to check the presence and validity of a user's arguments to a Python
program themselves whilst also automatically generating a friendly interface to
the program for the end user based on the definitions provided by the developer.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Testing}
\label{sec:part1:testing}

As discussed in Chapter~\ref{chap:methodology}, testing forms a critical part of
the project given the need to monitor the impact of changes to classification
accuracy as well as to ensure the program is working correctly. Ideally,
execution of a test suite should be simple and easily repeatable. Results that
pertain to accuracy should also be stored for future reference to monitor
ongoing performance of the classifier.

Such requirements could be fulfilled by a continuous integration platform -- a
server dedicated to the building and testing of the code contained in a
centralised repository typically to which an entire team will have write
access\citep{fowler-ci}. Whilst in this scenario there will be much less "risk"
from integration issues due to the single person team size, the themes of
automated building and self-testing code can still be taken on board.

\textbf{Jenkins}\citep{Jenkins} is a highly popular\cite{jenkins-stats} example
of such a platform with which I am familiar. Although an out-of-the-box
\textbf{Jenkins} instance is suitable for variety of software engineering
projects, it would be necessary to invest some time to install and tweak plugins
to perform actions on test results (such as failing a build that causes accuracy
to decrease).  Previous experience found that more specific tasks will often
require a plugin to be authored to overcome limitations in the feature set of a
more generic plugin, which given the intricacies of the \textbf{Jenkins} package
layout could easily turn into a project of its own. Unfortunately, other
features that would be useful to the project including the indexing and
searching of logs are somewhat lacking in \textbf{Jenkins}.

Online solutions such as \textbf{Travis}\citep{travis-ci} and
\textbf{Wercker}\citep{wercker} are free for open source projects and could
potentially offer a quicker set up, as both services merely require a small
configuration file in the root of the repository and a hook to be registered
(allowing builds to be triggered on a code push for example) before being able
to deploy a build in the cloud.

However these online services would not have been able to handle the return of
build artifacts (such as logs, graphs or dot files containing a decision tree)
without some convoluted solution of uploading them to another service or
committing them to a private code repository during execution of the job -- the
non-persistent nature of these nodes would cause any artifacts to be destroyed
when the node is terminated at the end of the build.

Also, as these virtual nodes are isolated from any sort of persisting file
system it would be necessary to upload large quantities of training data any
time a build and test job is to be run. This sounds rather inefficient in terms
of both bandwidth and time and would more than likely constitute unreasonable
use in view of the platform's "fair use" terms and conditions too.

It would seem that none of the widely available solutions would provide an
environment suitable for the testing of this project. I wanted to provide my
own platform for this problem but given the time constraints of the project this
was simply not practical and in the end we settled for well formatted log files
which could be searched and processed with command line tools such as
\textbf{awk}.


