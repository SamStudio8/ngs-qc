\chapter{Frontier}
\ifpdf
    \graphicspath{{Chapter3/Figs/Raster/}{Chapter3/Figs/PDF/}{Chapter3/Figs/}}
\else
    \graphicspath{{Chapter3/Figs/Vector/}{Chapter3/Figs/}}
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This chapter introduces \textbf{Frontier}: the main programming effort for this
part of the project. Frontier is a Python package that serves as a data manager,
providing both interfaces to read inputs into structures in memory and to
retrieve them in formats acceptable to a machine learning framework.

Appendix~\ref{app:pre-frontier} describes some analysis performed before
implementation of \textbf{Frontier}.

\section{Design}
\subsection{Purpose}

Frontier's purpose is to supplement analysis with \textbf{scikit-learn} by
allowing a user to read in and parameterise data in a format that can then be
used for analysis by the \textbf{scikit-learn} library.  Frontier was designed
to simplify the process of setting up machine learning tasks and enable
experiment repeatability by removing the need for users to spend time
constructing classes and functions to parse their input data and to just get on
with analysis using \textbf{scikit-learn}.

Initially Frontier was to act as a wrapper around \textbf{scikit-learn},
essentially removing the end user's interaction with the library and merely
providing an interface for data to be passed in and some sort of classifier to
be returned. However this quite clearly limited Frontier's audience by tying it
to a particular framework and would quickly become unmanageable in the task of
providing wrappers for all aspects of an external library.

Instead it was decided that Frontier would be used alongside a user's chosen
machine learning framework, providing a useful API to parse and extract
observations and variables from input data and arrange them in structures
suitable for processing with \textbf{scikit-learn}.

If possible, Frontier could also handle any objects returned, displaying or
logging any textual or graphical information pertaining to a returned
classifier's accuracy to assist a user in the ongoing performance monitoring of
changes to used data subsets or parameters.


\subsection{Format}

Frontier is designed as a Python package, allowing a user to import its
functionality in to other programs. The result of this project's technical
output could almost be considered as two separate entities: Frontier itself, the
package designed to ease user interaction with scikit-learn and \textbf{Front},
a Python script which implements Frontier's functionality in order to interact
with scikit-learn to conduct analysis on the \textbf{auto\_qc} data.


\subsection{Method}
\label{sec:frontier-method}

\textbf{Frontier} was the result of abstracting code and tools created during
the use of \textbf{scikit-learn} for analysis of the current \textbf{auto\_qc}
system. Whilst scripts initially hard-coded to suit the specific needs of the
project -- with hard coded classes and encodings -- \textbf{Frontier} followed
an evolutionary design process and grew to become a more generic framework
applicable to learning tasks outside of this study.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Concepts}

Appendix~\ref{app:concepts-p1} introduces some ideas that will assist
understanding of the application's purpose and terminology used in the following
implementation section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation}

This section investigates the implementation of Frontier's major components:

\begin{itemize}
    \item Informing the package of the problem domain...
    \item Interfaces provided for reading in data
    \item Storage of read data in memory
    \item Retrieval of stored data for interaction with \textbf{scikit-learn}
\end{itemize}


\subsection{Class Definitions}
\label{chap:classes}

Frontier was designed to support classification machine learning problems, to
adequately support this task, the package must be aware of each of the possible
classes in the problem space. Early versions of Frontier were specifically
designed for training and testing data from \textbf{auto\_qc} and could only
support encoding and decoding of the pass, fail and warn classes.

However this implementation was clearly esoteric and held little to no further
use outside the domain of the project's learning task. What would happen if a
class label were to be added or removed in future? Most likely many lines of
code would need to be re-written to handle such cases; the package was
inflexible.

To counter this, Frontier was refactored to remove hard coded label definitions
enabling its use as a more general purpose tool where users can specify the
domain's classes and their associated labels and encodings.
Listing~\ref{list:FrontierClasses} shows the definitions used by Front to work
with the \textbf{auto\_qc} data.

\begin{listing}[H]
    \caption[FrontierClasses]{: Class definitions for \textbf{auto\_qc} as passed to Frontier}
    \label{list:FrontierClasses}
    \begin{minted}[mathescape,
                %linenos,
                numbersep=5pt,
                gobble=8,
                frame=lines,
                framesep=2mm]{python}
        CLASSES = {
                "pass": {
                    "names": ["pass", "passed"],
                    "code": 1,
                },
                "fail": {
                    "names": ["fail", "failed"],
                    "code": -1,
                },
                "warn": {
                    "names": ["warn", "warning"],
                    "code": 0,
                },
        }
    \end{minted}
\end{listing}

As per Listing~\ref{list:FrontierClasses}, to define classes a user must provide
a Python dict containing the following for each label:

\begin{itemize}
    \item \textbf{class} \textit{String}\hfill\\
        The dictionary key is used as the canonical name of the class label
    \item \textbf{names} \textit{[String]}\hfill\\
        A list of labels that denote membership of this class
    \item \textbf{code} \textit{Integer}\hfill\\
        The encoded representation of this class (See Chapter~\ref{chap:labelcode})
    \item \textbf{\_recode} \textit{Boolean, Private}\hfill\\
        A flag to indicate whether an API action has changed the code of this
        class, typically used when treating all members of one class as a member
        of another -- A user should never set this manually
    \item \textbf{\_count} \textit{Integer, Private}\hfill\\
        The number of observations with this label, typically used when
        calculating weightings based on the proportions of class sizes and
        outputting logging information -- A user should not set this manually
\end{itemize}

This simple structure allows Frontier to be compatible with almost any
classification learning task with minimum input from the end user. Additional
utilities provided by the Frontier utils subpackage use this structure to
automatically classify and encode labels without user intervention with the
following functions:

\begin{itemize}
    \item \textbf{classify\_label} \hfill\\
        Attempt to classify a label by comparing a given string to each set of
        \textit{names}, locating an exact match will return the relevant
        canonical class label
    \item \textbf{encode\_class} \hfill\\
        Given a canonical class label, return its \textit{code}
    \item \textbf{decode\_class} \hfill\\
        Given a \textit{code}, return the canonical class label unless
        \textit{\_recode} is True
    \item \textbf{count\_class} \hfill\\
        Increment the \textit{\_count} for a particular class given its
        canonical label
\end{itemize}

These functions are put to use throughout Frontier and are essential for reader
classes (detailed in the next chapter) to parse, classify and then encode
observation targets automatically from relevant files.


\subsection{Input Handling}
\label{sec:frontier-input}

Frontier's modular nature allows users to write their own Python classes to read
data from any form of input file or stream. Two examples of which are the
classes used to read from the "bamcheckr'd files" documented in
Appendix~\ref{app:bamcheckr} and the \textbf{auto\_qc} decisions matrix briefly
demonstrated in Appendix~\ref{app:aqc_matrix}.

These classes are described as \textbf{Readers} and implement a common base
class, \textbf{AbstractReader} which takes care of
setting up the file handler, including functions to both close and iterate over
the file's contents. It will also automatically call its own
\textbf{process\_file} function that skips over any header lines before passing
each line in the file stripped of any newline characters to \textbf{process\_line}.

It is expected that derived classes will at least provide their own
implementations for \textbf{process\_line} and \textbf{get\_data}. Failing to do
so will cause Frontier to throw a \textbf{NotImplementedError} when attempting
to use the class to read data.

\textbf{process\_line} defines the line handling operations that extract and store
desired data found in a given line. This responsibility includes returning None
for lines that contain comments and irrelevant data and conducting any necessary
sanitisation (the \textbf{BamcheckReader} for example makes use of a utility
function to translate spaces, underscores and dots to hyphens).

\textbf{get\_data} must return any read in data in a suitable structure for
storage by Frontier. Typically this will be a Python dictionary using some
unique identifier for each observation as a key, mapping to an arbitrary value
or object containing that observation's parameters. Further discussion on
Frontier's storage of data and targets is to follow.

The \textbf{AbstractReader} class is designed to simplify the process of reading
in observations and their targets for end users, however it is still up to the
author of the dervied class to set up any structures to store data (which cannot
be done automatically without likely enforcing potentially unhelpful
constraints) before initialisation of the inherited base class (via the call to
\textbf{super}) as shown in Listing~\ref{list:superreader}.

\begin{listing}[H]
    \caption[superreader]{: Extract from \textbf{BamcheckReader} class
        documenting initialisation of necessary data structures and calling
        for initialisation of its inherited base class}
    \label{list:superreader}
    \begin{minted}[mathescape,
                %linenos,
                numbersep=5pt,
                gobble=8,
                frame=lines,
                framesep=2mm]{python}
        class BamcheckReader(AbstractReader):
            [...]

            def __init__(self, filepath, CLASSES, auto_close=True):
                self.summary = SummaryNumbers()
                self.indel = IndelDistribution()
                super(BamcheckReader, self).__init__(filepath, CLASSES, auto_close, 0)

            [...]
    \end{minted}
\end{listing}

As shown in Listing~\ref{list:superreader}, the initialisation of the
\textbf{AbstractReader} allows four arguments:

\begin{itemize}
    \item \textbf{filepath} \textit{String}\hfill\\
        A relative or absolute path to a file from which to extract data or
        targets
    \item \textbf{CLASSES} \textit{Dictionary}\hfill\\
        A dictionary of user specified class labels defined as described in
        the previous chapter
    \item \textbf{auto\_close} \textit{Boolean, Optional}\hfill\\
        Whether or not to close the file handle immediately after executing
        \textbf{process\_file}, this is True by default to prevent either memory
        leaking when users are reading in a large number of files and are
        perhaps unaware that they require closing or the throwing of an IOError
        caused by having too many file handles open at once
    \item \textbf{header} \textit{Integer, Optional}\hfill\\
        The number of lines to ignore before the reader should begin
        passing stripped lines to \textbf{process\_line}, defaults to 0
\end{itemize}

Currently it is also the responsibility of the author of a derived class to
perform relevant sanity checking of any extracted data. For example the
\textbf{BamcheckReader} class checks for the presence of multiple entries of a
particular metric which will print a notice if found, unless the entries have
differing values, upon which an exception is thrown and the process is halted.

Once a reader has been defined for a particular file format, a user need only
provide a directory of files to be parsed and the name of the class designed to
complete the parsing. Frontier will then take care of executing the parsing
process on all files in the directory. After a derived reader has completed file
handling, Frontier will call its \textbf{get\_data} function to
"move"\footnote{Rather, a pointer to the address of the extracted data's
dictionary hashmap will be copied to memory inside a Frontier class} the
extracted data to its own storage.

%TODO Cite algorithm
At this point Frontier will also check the integrity of the data, primarily that
all parameters have a non-zero variance. Users will be warned when this
requirement is violated; parameters with no variance cannot provide much
information for successful classification as their values are equal for all
class labels!

%FUTURE
In future it would be useful to investigate whether it would be feasible to
perform such sanity checking in a generic manner to ensure it could be applied
to a wide enough range of scenarios to make it worth including functionality in
the \textbf{AbstractReader} directly.

With more time, future improvements could overhaul the reader interfaces
to allow users to simply specify the format of a file in a string that can be
parsed by Frontier's IO subpackage, rather than having to write their own derived
class. Classes could also list file extensions they are capable of processing
which could potentially be used to automatically determine which readers to use
without requiring the user to explicitly specify.


\subsection{Storage}

Frontier specifies a class called the \textbf{Statplexer}\footnote{A somewhat
contrived contraction of 'Statistics Multiplexer'} which provides users with a
single point of access to all read in data. The reader interfaces described in
the previous chapter implement their own loading functions to populate the
\textbf{\_data} and \textbf{\_targets} class members of the \textbf{Statplexer}
object, both of which are Python dictionaries.

During the parsing of observation data with the relevant reader class, the
reader is expected to locate an appropriate unique ID for each observation. In
the case of processing of \textbf{auto\_qc} data, this would be the lanelet's
barcode which is collected from the filename of that particular lanelet's
"bamcheckr'd" file.
This identifier is then used as a key in both the \textbf{\_data} and
\textbf{\_targets} dictionaries to map to a structure (typically another
dictionary or an arbitrary class) that stores that observation's parameters
and its known target (encoded class label), respectively.

Although these attributes can be manipulated directly (and indeed they are for
testing purposes) the leading underscore follows a popular convention defined in
Python's style guideline, PEP8\citep{pep8}, where class members with leading
underscores should be treated as non-public. Python doesn't have private
variables such as those that may be found in other languages like Java, indeed
the Python style guide points out that "no attribute is really private in
Python"\citep{pep8}. In an interesting StackOverflow answer on the subject, a
user describes that this is "cultural"\citep{so:pythonprivate} and that Python
programmers are trusted not to circumvent convention and "mess around with those
private members". Users are therefore expected to use the functionality
Frontier provides for controlled getting and setting of data stored in these
pseudo-private \textbf{\_data} and \textbf{\_targets} members.

\textbf{load\_data}, for example, should be used exclusively when populating
the two dictionaries and is automatically called on construction of the
\textbf{Statplexer} if the construction arguments are valid.
\textbf{load\_data} will automatically call other necessary (pseudo-private)
functions of the class including \textbf{\_test\_variance} which checks that
parameter variances are non-zero and also warns users if new observations
are overwriting old ones, or if an observation does not appear to have a
corresponding target.

The following section details the retrieval of data and targets from the
\textbf{Statplexer} which are returned to the user in \textbf{NumPy} arrays. Why
does \textbf{Frontier} not just store the read in data in such a structure to
begin with?

This is primarily due to the underlying layout of a list structure as introduced
in Section~\ref{sec:python-structures}. Given the nature of input data,
the number of observations and parameters are unknown before the input
data is actually read and so memory cannot be reserved to prevent these operations.
The potential number of input files renders reading through the data once to
reserve the right amount of memory for a second read-through impractical in
terms of time.

It is arguable that despite this, the data could be unloaded from the
\textbf{\_data} and \textbf{\_targets} dictionaries in to some form of array at
the end of the call to \textbf{load\_data}. However as the \textbf{Statplexer}
allows loading of additional data at any time, which would not only risk
increasingly expensive resizing operations as the list continues to expand but
the sanity checking that takes place in \textbf{load\_data} involves checking
for membership of a given ID in \textbf{\_data} and \textbf{\_targets} which is
constant for dictionaries and significantly slower for large lists (see
Section~\ref{sec:python-structures}).

It is possible that an implementation could use both a dictionary and an array;
appending the observations to the array and entering a mapping between the
observation ID and its index in the array.  Though this would still not avoid
the issue of relocating the array once it grows beyond its bound in memory.

Worse still, as results requested from the \textbf{Statplexer} API are returned
to the user in a sorted matrix\footnote{A two-dimensional \textbf{NumPy} array}
(\textit{i.e.} rows and columns are ordered alphanumerically by the observation
ID and parameter name, respectively), if observations are not processed in a
sorted order (which is not a requirement) then returning results will involve
accessing the proposed \textbf{\_data} and \textbf{\_targets} arrays in a
non-linear fashion, losing any efficiency that could be gained from using an
array in the first place.

Whilst not entirely optimal\footnote{Dictionaries still require use of
\textbf{sorted} when creating a result matrix via the API}, in terms of a
compromise between memory complexity and avoiding expensive operations on data
input and retrieval, dictionaries offer the best balance.

It should be noted that the \textbf{Statplexer} stores a copy of the user
defined CLASSES dictionary (as the class member \textbf{\_classes}), which is an
argument to its construction, allowing the \textbf{Statplexer} to share this
information with any readers or utility functions who require it.


\subsection{Retrieval}

\textbf{Frontier}'s \textbf{Statplexer} is designed to provide functions to an
end user for extracting desired data in a format suitable for passing as an
argument to functions of an external framework or library, such as
\textbf{scikit-learn}. As specified in \textbf{Frontier}'s purpose, the package
must provide user-friendly functions to extract observations and parameters of
interest from the internal \textbf{Statplexer} representation.

The previous section suggests that directly accessing elements of the
\textbf{Statplexer}'s \textbf{\_data} and \textbf{\_targets} member dictionaries,
whilst possible, would violate the psuedo-private nature of the variables and even
so, this would hardly be a user friendly way in which to obtain data from the
\textbf{Statplexer}.

As described in Section~\ref{sec:frontier-method}, development of
\textbf{Frontier} was evolutionary, growing to meet the needs and requirements
of the underlying machine learning problem that this project strives to solve.
It became clear that in trying many combinations of observation and parameter
subsets that \textbf{Frontier} would need to provide not just a function to
return the contents of \textbf{\_data} and \textbf{\_targets} but to assist in
retreiving similar subsets automatically with as little effort from the end user
as possible.

But how can a user make an informed decision on parameters to subset?
Though this is primarily the role of feature selection, which will be discussed
later in this chapter, firstly a user will need to have a feeling for what
parameters are actually present and ideally be offered functionality to
allow for quick elementary exploration of that set. For this, \textbf{Frontier}
can be used to inspect the parameters extracted from the observations via the
functions:

\begin{itemize}
    \item \textbf{list\_parameters} \hfill\\
        Return a sorted list of all parameters
    \item \textbf{find\_parameters} \hfill\\
        Given a list of input strings, return a list of parameters which contain
        any of those strings as a substring
    \item \textbf{exclude\_parameters} \hfill\\
        Given a list of input strings, return a list of parameters which do not
        contain any of the input strings as a substring, or if needed an exact
        match
\end{itemize}

Once users have an idea of the parameters they wish to extract from each
observation, \textbf{Frontier} maintains an additional set of functions that act
as a form of API, allowing users to retrieve subsets of data based on both
parameters and targets:

\begin{itemize}
    \item \textbf{get\_data\_by\_parameters} \hfill\\
        Return data for each observation, but only include columns
        for each parameter in the given list
    \item \textbf{get\_data\_by\_target} \hfill\\
        Return data for each observation that have been classified in one of the
        targets specified and additionally only return columns for the
        parameters in the given list
\end{itemize}

%TODO Figure to show mapping between container elements
The prior section first introduces the format in which data is returned, the
two-dimensional \textbf{NumPy} array -- referred to as the \textit{Frontier Results
Matrix} -- in which a row represents a particular observation and each column
represents a parameter (or feature). By default both dimensions are
ordered alphanumerically; rows are sorted by their observation ID, columns by
the parameter name\footnote{Typically the 'cleaned' or otherwise sanitised
    parameter name, as noted in Section~\ref{sec:frontier-input} for example
where the \textbf{BamcheckReader} class will automatically convert spaces and
other predefined characters to hyphens.}. Targets are also returned in a
\textbf{NumPy} array, with each index of the target array mapping 1:1 with the
observation row of the same index in the \textit{Results Matrix}.


\subsection{Logging}

Section~\ref{sec:part1:testing} concluded that time constraints would not allow
significant effort to be invested in building a platform for cataloguing results
from test runs of the classifier. I decided to settle for populating fields in a
structured text file, an example of which can be found in
Appendix~\ref{app:frontier-log}.

\textbf{Frontier} provides functions such as \textbf{write\_log} to dump
\textbf{Statplexer} parameters to a given file path along with cross-validation
performance associated with a particular training and prediction run of a
decision tree classifier. Such files can easily be manipulated with common
command line tools such as \textbf{grep} and the text processing language,
\textbf{awk}.

Given the time to create specialised software for storing and comparing
different runs of the classifier in future, it would be trivial to import old
\textbf{Frontier} log files.


\section{Usage Example}

\begin{listing}[H]
    \caption[callstatplexer]{: Example usage of Frontier}
    \label{list:callstatplexer}
    \begin{minted}[mathescape,
                %linenos,
                numbersep=5pt,
                gobble=8,
                frame=lines,
                framesep=2mm]{python}

        from Frontier import frontier
        from Frontier.IO import DataReader, TargetReader

        data_dir = "/home/sam/Projects/owl_classifier/data/"
        target_path = "/home/sam/Projects/owl_classifier/targets.txt"

        CLASSES = {
                "hoot": {
                    "names": ["owl", "owls"],
                    "code": 1,
                },
                "unhoot": {
                    "names": ["cat", "dog", "pancake"],
                    "code": 0,
                },
        }

        statplexer = frontier.Statplexer(data_dir,
                                         target_path,
                                         CLASSES,
                                         DataReader,
                                         TargetReader)
    \end{minted}
\end{listing}

Constructing the Statplexer requires the following arguments:

\begin{itemize}
    \item \textbf{data\_dir} \textit{String}\hfill\\
        Root data directory under which all files will be parsed for observation data
    \item \textbf{target\_path} \textit{String}\hfill\\
        Path to file to be parsed for observation targets
    \item \textbf{CLASSES} \textit{Dictionary}\hfill\\
        A dictionary of user specified class labels defined as described in
        Chapter~\ref{chap:classes}
    \item \textbf{DataReader} \textit{Module}\hfill\\
        Module containing the class (of the same name) to be used to parse each
        file in the \textit{data\_dir} tree for observation data
    \item \textbf{TargetReader} \textit{Module}\hfill\\
        Module containing the class (of the same name) to be used to parse the
        \textit{target\_path} file for target data
\end{itemize}


\section{Testing}

\textbf{Frontier} is bundled with a small testing suite designed to flex the
functionality of the two included readers (\textbf{AQCReader} and
\textbf{BamcheckReader}) as well as both the \textbf{Frontier} utilities and
the \textbf{Frontier} API itself. The suite consists of three Python scripts
which utilise the \textbf{unittest}\citep{py-unittest} package in the Python
standard library.  The scripts are located in the \textit{tests} directory of
the \textbf{Frontier} package.

Listing~\ref{list:frontier-test} displays the commands necessary to execute each
test script. Note the \texttt{-m} option to the \texttt{Python} command
instructs the interpreter to load the module named as a script.

\begin{listing}[H]
    \caption[frontier-test]{\textbf{Execution of the Frontier Testing Suite}}
    \label{list:frontier-test}
    \begin{minted}[mathescape,
                %linenos,
                numbersep=5pt,
                gobble=8,
                frame=lines,
                framesep=2mm]{bash}

        python -m tests/frontier
        python -m tests/aqcreader
        python -m tests/bamcheckreader
    \end{minted}
\end{listing}

\subsection{AQCReader and BamcheckReader}

Both of \textbf{Frontier}'s included readers are distributed with their own
modest test suite, designed to test the particular functionality of that reader.


* Test cleaning of data
* Test duplicates warning
* Test duplicates exception
* BC Tests that some hardcoded expected values are returned
* AQC Tests that number of labels were as expected...

\subsection{Frontier Utilities}

* Test both encoding and decoding of class labels and class codes
* Ensures that exceptions are thrown when unknown class labels or codes are
encountered (as this should not happen!)

\subsection{Frontier API}
* More complex
* Sets up a series of test observations and parameters
* Ensures each API function returns the data expected
* ...including when parameters are incorrect or unknown
* ...ensures parameters should be excluded or included...
* actually found a problem with excluding!
* Ensures targets map correctly to data


