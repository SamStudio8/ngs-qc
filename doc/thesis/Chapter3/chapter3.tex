\chapter{Frontier}
\ifpdf
    \graphicspath{{Chapter3/Figs/Raster/}{Chapter3/Figs/PDF/}{Chapter3/Figs/}}
\else
    \graphicspath{{Chapter3/Figs/Vector/}{Chapter3/Figs/}}
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%TODO Better description of Frontier
This chapter introduces \textbf{Frontier}: the main programming effort for this
part of the project. Frontier is a Python package that serves as a data manager,
providing both interfaces to read inputs into structures in memory and to
retrieve them in formats acceptable to a machine learning framework.

\section{Design}
\subsection{Purpose}

Frontier's purpose is to supplement analysis with \textbf{scikit-learn} by
allowing a user to read in and parameterise data in a format that can then be
used for analysis by the \textbf{scikit-learn} library.  Frontier was designed
to simplify the process of setting up machine learning tasks and enable
experiment repeatability by removing the need for users to spend time
constructing classes and functions to parse their input data and to just get on
with analysis using \textbf{scikit-learn}.

Initially Frontier was to act as a wrapper around \textbf{scikit-learn},
essentially removing the end user's interaction with the library and merely
providing an interface for data to be passed in and some sort of classifier to
be returned. However this quite clearly limited Frontier's audience by tying it
to a particular framework and would quickly become unmanageable in the task of
providing wrappers for all aspects of an external library.

Instead it was decided that Frontier would be used alongside a user's chosen
machine learning framework, providing a useful API to parse and extract
observations and variables from input data and arrange them in structures
suitable for processing with \textbf{scikit-learn}.

If possible, Frontier could also handle any objects returned, displaying or
logging any textual or graphical information pertaining to a returned
classifier's accuracy to assist a user in the ongoing performance monitoring of
changes to used data subsets or parameters.


\subsection{Format}

Frontier is designed as a Python package, allowing a user to import its
functionality in to other programs. The result of this project's technical
output could almost be considered as two seperate entities: Frontier itself, the
package designed to ease user interaction with scikit-learn and \textbf{Front},
a Python script which implements Frontier's functionality in order to interact
with scikit-learn to conduct analysis on the \textbf{auto\_qc} data.


\subsection{Method}

.. the result of abstracting code and tools created during the use of
\textbf{scikit-learn} for analysis of the current \textbf{auto\_qc} system


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Concepts}

...introduce some terminology and ideas that will assist understanding of the
following implemenetation section...

\subsection{Observations and Parameters}

...designed to work in a machine learning environment...
...\textit{observation} refers to an identifiable "thing" from which we will
attempt to learn... in the context of this project, a lanelet
...\textit{parameter} or \textit{regressor} will typically be used
interchangeably to refer to attributes of an observation which will be analysed
for potential relationships to the observation target
...it is expected that each observation will have the same parameters available


\subsection{Data and Targets}

...\textit{data} will be used somewhat generically to refer to a matrix in which
observations act as rows and their parameters act as columns...
...An observation's \textit{target} is the known classification of that
observation, depending on the context of the learning problem these may be
discovered manually (for example if trying to count leaves from images, one may
have to manually count the leaves in an image) or the output of another system
may be used; in this case the target's are from auto\_qc...

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation}
...we investigate the following implementation stories:

\begin{itemize}
    \item User input, readers, sanity checking
    \item Storage of read data in memory
    \item Retrieval
    \item Interaction with scikit-learn
\end{itemize}


\subsection{Input Handling}

Frontier's modular nature allows users to write their own Python classes to read
data from any form of input. Two examples of which are the provided classes used
to read from the "bamcheckr'd files" as seen in Appendix~\ref{app:bamcheckr} and
the auto\_qc output matrix briefly demonstrated in Appendix~\ref{app:aqc_matrix}.

.. initially hardcoded to suit the specific needs of the project but followed
an evolutionary design process...

designed specically for the given learning problem, with hard coded
classes and encodings...

...a major refactoring to remove hard-coded classes from \textbf{Frontier} which
enable it to be used as a more general purpose tool, users now specify the
classes and associated labels and encodings (for example those used by Front are
demonstrated in Listing~\ref{fig:FrontierClasses})...

...if we were to add another
class label, the definition would merely need to be included to the CLASSES
(Listing~\ref{fig:FrontierClasses}) variable passed when the Statplexer is
constructed. But use is therefore not merely limited to our problem but rather
any machine learning problem where you'd like to simplify your interactions
which a very large dataset.

\begin{listing}[H]
    \caption[FrontierClasses]{Class definitions for auto\_qc as passed to Frontier}
    \label{fig:FrontierClasses}
    \begin{minted}[mathescape,
                %linenos,
                numbersep=5pt,
                gobble=8,
                frame=lines,
                framesep=2mm]{python}
        CLASSES = {
                "pass": {
                    "class": ["pass"],
                    "names": ["pass", "passed"],
                    "code": 1,
                },
                "fail": {
                    "class": ["fail"],
                    "names": ["fail", "failed"],
                    "code": -1,
                },
                "warn": {
                    "class": ["warn"],
                    "names": ["warn", "warning"],
                    "code": 0,
                },
        }
    \end{minted}
\end{listing}

...this allows Frontier to be compatable with almost any learning task, you
merely specify the correct data and target readers, the known classes and call
the \textbf{Statplexer}...

% Cite algorithm
...Frontier also checks each parameter for variance, warning users when this
requirement is violated... (a parameter with 0 variance is pretty useless...)

\subsection{Storage}

% TODO Cite tests?
Frontier specifies a class called the \textbf{Statplexer}\footnote{A somewhat
contrived contraction of 'Statistics Multiplexer'} which provides users with a
single point of access to all read in data. The reader interfaces described in
the previous chapter implement their own loading functions which populate the
\textbf{\_data} and \textbf{\_targets} class members of the \textbf{Statplexer}
object.

Although these attributes can be manipulated directly (and indeed they are for
testing purposes) the leading underscore follows a popular convention defined in
Python's style guideline, PEP8\citep{pep8}, where class members with such
leading underscores should be treated as non-public. Python doesn't have private
variables such as those that may be found in other languages like Java, indeed
the Python style guide points out that "no attribute is really private in
Python"\citep{pep8}. In an interesting StackOverflow answer on the subject, a
user describes that this is "cultural"\citep{so:pythonprivate} and that Python
programmers are trusted not "mess around with those private members".

Frontier provides functions to allow controlled access to get and set data
stored in these psuedo-private \textbf{\_data} and \textbf{\_targets} members.

%TODO Find better citation (Python book)
...\textbf{\_data} and \textbf{\_targets} are Python dictionaries, a structure
in which hashed keys are mapped to an arbitrary value or object. Python
dictionaries offer $O$(1) (constant) lookup\citep{py:timecomplexity} access
...more importantly querying the structure for whether it contains a particular
key can also be performed in constant $O$(1) as opposed to searching a list for
membership...

...when input data is parsed, the relevant reader class is expected to locate an
appropriate id for an observation, in our case, a sample's label...
...this id is used as a key for both the \textbf{\_data} and \textbf{\_targets}
dictionaries, which map this id to some structure containing that observation's
parameter values and classification respectively...

...why not a list?
...numpy lists also require the length to be known when they are created which
would not be possible without reading through all the input files first,
somewhat of a waste of time...

...primarily because the number of inputs is unknown, a list structure is
represented as a contiguous array in memory, causing very expensive copying
operations when the list needs to be resized...

...could we not load data in to a list at the end of input file reading?
...the load\_data function allows data to be loaded in to the
\textbf{Statplexer} at any time, thus requiring use of the expensive memory
copying operations when the list once again needs to be resized...


...an issue with use of a dictionary however is by default they are unsorted,
not only this but the order in which the keys are iterated over will be
undefined...
...currently this causes the list of keys to require sorting each time a user
queries the Statplexer API for data... as obviously the parameters and targets
must map one-to-one in a predictable and repeatable manner.
%TODO Figure to show mapping between container elements

...through testing of the 13,455 observations it would seem that this sorting
operation is inexpensive... and indeed seems a fair price to pay for having
constant lookup speed on membership as well as being able to access an object by
its id rather than knowing where it is in a list...

...although the latter point is somewhat easily fixed for an array
implementation by using a dictionary to provide a mapping between a sample name
and where it is stored in an array...

...also storage of classes (used for access to Frontier utils)
...utils to classify a label, encode a class, decode a class and count a
class...


\subsection{Retrieval}
The Statplexer is designed to provide the user methods in which to extract
desired data in a format suitable for parsing with an external framework or
library, in our case; scikit-learn

...leverages \textbf{numpy}...
...some of the methods provided include:

\begin{description}
    \item[list\_regressors] Retrieve a list of all parameters
    \item[find\_regressors] Retrieve a list of parameters containing any of the
        input strings as a substring
    \item[exclude\_regressors] Retrieve a list of parameters which do not
        contain any of the input strings as a substring, or if needed an exact
        match
\end{description}

...data is returned sorted by key, with parameters sorted alphanumerically...
...targets map 1:1 with the data array...

\begin{description}
    \item[get\_data\_by\_regressors] Return data for each observation, but only
        return columns for the parameters in the given list
    \item[get\_data\_by\_target] Return data for each observation that have been
        classified in one of the targets specified and additionally only
        return columns for the parameters in the given list
\end{description}

\subsection{Interaction with scikit-learn}
\subsubsection{Cross Validation}

...method in which to measure classification accuracy...
...potentially use a weighting to penalise mistakes in smaller classes...

...K fold cross validation
...using stratified K fold cross validation...

\subsubsection{Confusion Matrices}
"Normal" confusion matrix and "Warnings" confusion matrix...

\subsection{Parameter Selection}
...important to find the "best" parameters
...what is best? scikit-learn uses total gini information

...frontier uses two methods:
\begin{itemize}
    \item Backward elimination; pruning parameters with the lowest total gini
    \item Call scikit-learn's SelectKBest
\end{itemize}


* incorrect degrees of freedom
* warnings: /usr/lib64/python2.7/site-packages/sklearn/feature\_selection/univariate\_selection.py:256: RuntimeWarning: invalid value encountered in divide, causing NaN
* Replaced univariate\_selection with version from master
* needed use force np.float64
...actually data was 0... gg :(

\section{Testing}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Contributions to bamcheckr}
\begin{minted}[mathescape,
               %linenos,
               numbersep=5pt,
               gobble=4,
               frame=lines,
               framesep=2mm]{r}
    install.packages("devtools")
    library(devtools)

    # Install directly from github repository
    install_github("samstudio8/seq_autoqc", subdir="bamcheckr")

    # Install from local directory
    install("/home/sam/Projects/seq_autoqc/bamcheckr")

\end{minted}
Takes 5.5s on average, 16.1s with ratio due to inefficient implementation
for overlapping\_base\_duplicate\_percentage
re-wrote in Python...


...R CMD BATCH issue

...Fixed a graph plotting failure.
...Writing additional routines...


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

